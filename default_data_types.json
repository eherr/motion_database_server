{
    "tags": [
        "skeleton_motion",
        "model",
        "processed",
        "clip",
        "compressed_bson",
        "raw"
    ],
    "data_types": {
        "aligned_motion": {
            "name": "aligned_motion",
            "requirements": "",
            "tags": [
                "skeleton_motion",
                "processed",
                "clip"
            ]
        },
        "mm": {
            "name": "mm",
            "requirements": "pip install git+https://github.com/eherr/motion-matching",
            "tags": [
                "model",
                "skeleton_motion"
            ]
        },
        "motion": {
            "name": "motion",
            "requirements": "pip install git+https://github.com/eherr/anim_utils",
            "tags": [
                "skeleton_motion",
                "compressed_bson",
                "clip",
                "raw"
            ]
        },
        "mpm": {
            "name": "mpm",
            "requirements": "pip install git+https://github.com/dfki-asr/morphablegraphs",
            "tags": [
                "compressed_bson",
                "model",
                "skeleton_motion"
            ]
        },
        "mpmg": {
            "name": "mpmg",
            "requirements": "pip install git+https://github.com/dfki-asr/morphablegraphs",
            "tags": [
                "skeleton_motion",
                "model"
            ]
        },
        "sb3": {
            "name": "sb3",
            "requirements": "pip install git+https://github.com/DLR-RM/stable-baselines3",
            "tags": [
                "model"
            ]
        }
    },
    "data_loaders": {
        "mpmg:vis_utils": {
            "dataType": "mpmg",
            "engine": "vis_utils",
            "script": "#!/usr/bin/env python\r\n#\r\n# Copyright 2019 DFKI GmbH.\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a\r\n# copy of this software and associated documentation files (the\r\n# \"Software\"), to deal in the Software without restriction, including\r\n# without limitation the rights to use, copy, modify, merge, publish,\r\n# distribute, sublicense, and/or sell copies of the Software, and to permit\r\n# persons to whom the Software is furnished to do so, subject to the\r\n# following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included\r\n# in all copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\r\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\r\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\r\n# NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\r\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\r\n# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\r\n# USE OR OTHER DEALINGS IN THE SOFTWARE.\r\nimport copy\r\nimport threading\r\nimport numpy as np\r\nfrom datetime import datetime\r\nimport os\r\nimport glob\r\nfrom anim_utils.animation_data.motion_state import MotionState\r\nfrom vis_utils.animation.state_machine_controller import StateMachineController\r\nfrom vis_utils.scene.scene_object_builder import SceneObjectBuilder, SceneObject\r\nfrom anim_utils.animation_data.motion_concatenation import align_joint\r\nfrom anim_utils.motion_editing import MotionGrounding\r\nfrom anim_utils.animation_data.bvh import BVHReader\r\nfrom anim_utils.animation_data.motion_vector import MotionVector\r\nfrom anim_utils.retargeting.analytical import Retargeting, generate_joint_map\r\nfrom anim_utils.utilities.log import set_log_mode, LOG_MODE_DEBUG\r\nfrom transformations import euler_matrix\r\nimport vis_utils.constants as constants\r\nfrom morphablegraphs.motion_generator.algorithm_configuration import DEFAULT_ALGORITHM_CONFIG\r\nfrom morphablegraphs.motion_model import NODE_TYPE_STANDARD, NODE_TYPE_IDLE\r\nfrom morphablegraphs.motion_model.static_motion_primitive import StaticMotionPrimitive\r\nfrom morphablegraphs.motion_generator.mg_state_planner import MGStatePlanner, get_node_aligning_2d_transform, ANIMATED_JOINTS_CUSTOM\r\nfrom morphablegraphs.constraints.constraint_builder import UnityFrameConstraint\r\nfrom morphablegraphs.motion_generator.mg_state_queue import StateQueueEntry\r\nfrom .simple_navigation_agent import SimpleNavigationAgent\r\nfrom vis_utils.scene.utils import get_random_color\r\nfrom morphablegraphs.motion_model.motion_state_graph_loader import MotionStateGraphLoader\r\n\r\ndef rotate_vector_deg(vec, a):\r\n    a = np.radians(a)\r\n    m = euler_matrix(0, a, 0)[:3,:3]\r\n    vec = np.dot(m, vec)\r\n    return vec\r\n\r\nDEFAULT_CONFIG = dict()\r\nDEFAULT_CONFIG[\"algorithm\"]  = DEFAULT_ALGORITHM_CONFIG\r\nDEFAULT_CONFIG[\"algorithm\"][\"n_random_samples\"] = 300\r\nDEFAULT_CONFIG[\"algorithm\"][\"n_cluster_search_candidates\"] = 4\r\nDEFAULT_CONFIG[\"algorithm\"][\"local_optimization_settings\"][\"max_iterations\"] = 1000\r\nDEFAULT_CONFIG[\"algorithm\"][\"local_optimization_settings\"][\"method\"] = \"L-BFGS-B\"\r\nDEFAULT_CONFIG[\"algorithm\"][\"local_optimization_mode\"] = \"none\"\r\nDEFAULT_CONFIG[\"algorithm\"][\"inverse_kinematics_settings\"][\"interpolation_window\"]= 120\r\nDEFAULT_CONFIG[\"algorithm\"][\"inverse_kinematics_settings\"][\"transition_window\"]= 60\r\n\r\ndef load_clips(data_path):\r\n    print(\"load clips\", data_path)\r\n    loaded_clips = dict()\r\n    for filepath in glob.glob(data_path+os.sep+\"*.bvh\"):\r\n        print(\"load\", filepath)\r\n        name = filepath.split(os.sep)[-1][:-4]\r\n        bvh = BVHReader(filepath)\r\n        mv = MotionVector()\r\n        mv.from_bvh_reader(bvh, True)\r\n        state = MotionState(mv)\r\n        state.play = True\r\n        print(\"load\", name, mv.n_frames)\r\n        loaded_clips[name] = state\r\n    return loaded_clips\r\n\r\n\r\ndef update_collision_objects(scene, collision_objects):\r\n    for o_desc in collision_objects:\r\n        o_id = o_desc[\"id\"]\r\n        p = [o_desc[\"position\"][\"x\"], o_desc[\"position\"][\"y\"], o_desc[\"position\"][\"z\"]]\r\n        q = [o_desc[\"rotation\"][\"w\"], o_desc[\"rotation\"][\"x\"], o_desc[\"rotation\"][\"y\"], o_desc[\"rotation\"][\"z\"]]\r\n        print(\"set position\", o_id, p)\r\n        o = scene.get_scene_node_by_name(str(o_id))\r\n        if o is not None:\r\n            print(\"found object\", o_id, p)\r\n            o.setPosition(p)\r\n            o.setQuaternion(q)\r\n        else:\r\n            print(\"could not find object\")\r\n\r\n\r\nclass MorphableGraphStateMachine(StateMachineController):\r\n    def __init__(self, scene_object, graph, start_node=None, use_all_joints=False, config=DEFAULT_CONFIG, pfnn_data=None):\r\n        StateMachineController.__init__(self, scene_object)\r\n        self._graph = graph\r\n        if start_node is None or start_node not in self._graph.nodes:\r\n            start_node = self._graph.start_node\r\n        self.start_node = start_node\r\n        self.frame_time = self._graph.skeleton.frame_time\r\n        self.skeleton = self._graph.skeleton\r\n        self.thread = None\r\n        self._visualization = None\r\n        set_log_mode(LOG_MODE_DEBUG)\r\n        self.current_node = self.start_node\r\n        self.use_all_joints = use_all_joints\r\n        self.node_type = NODE_TYPE_IDLE\r\n        self.state = None\r\n        self.set_initial_idle_state(use_all_joints)\r\n        print(\"start node\", self.current_node)\r\n        self.start_pose = {\"position\": [0, 0, 0], \"orientation\": [0, 0, 0]}\r\n        self.speed = 1\r\n        if \"speed\" in config:\r\n            self.speed = config[\"speed\"]\r\n        print(\"set speed\", self.speed)\r\n        \r\n        self.pose_buffer = []\r\n        self.buffer_size = 10\r\n        self.max_step_length = 80\r\n        self.direction_vector = np.array([-1.0, 0.0, 0.0])\r\n        self.action_constraint = None\r\n        self.target_projection_len = 0\r\n        self.n_joints = len(self.skeleton.animated_joints)\r\n        self.n_max_state_queries = 20\r\n\r\n        self.retarget_engine = None\r\n        self.target_skeleton = None\r\n        self.activate_emit = False\r\n        self.show_skeleton = True\r\n        self.node_queue = []\r\n        self.activate_grounding = False\r\n        self.collision_boundary = None\r\n        self.hand_collision_boundary = None\r\n\r\n        self.aligning_transform = np.eye(4)\r\n        self.draw_root_trajectory = False\r\n        self.planner = MGStatePlanner(self, self._graph, config)\r\n        self.motion_grounding = MotionGrounding(self.skeleton, config[\"algorithm\"][\"inverse_kinematics_settings\"], self.skeleton.skeleton_model)\r\n        self.actions = self.planner.action_definitions\r\n        self.planner.settings.use_all_joints = use_all_joints\r\n        self.state.play = True\r\n        self.thread = None\r\n        self.animation_server = None\r\n        self.success = True\r\n        self.is_recording = False\r\n        self.stop_current_state = False\r\n        self.lock = threading.Lock()\r\n        self.recorded_poses = list()\r\n        #if pfnn_data is not None:\r\n        #    self.planner.pfnn_wrapper = PFNNWrapper.load_from_dict(self.skeleton, pfnn_data[\"weights\"], pfnn_data[\"means\"])\r\n        #    self.planner.use_pfnn = True\r\n        self.load_clips = dict()\r\n        if \"clip_directory\" in config and \"clip_directory\" in config:\r\n            data_path = config[\"data_directory\"] + os.sep + config[\"clip_directory\"]\r\n            if os.path.isdir(data_path):\r\n                self.loaded_clips = load_clips(data_path)\r\n            else:\r\n                print(\"Could not find clip directory\", data_path)\r\n\r\n    def set_graph(self, graph, start_node):\r\n        print(\"set graph\")\r\n        self.lock.acquire()\r\n        self.stop_current_state = True\r\n        if self.thread is not None:\r\n            print(\"stop thread\")\r\n            self.planner.stop_thread = True\r\n            self.thread.join()\r\n            self.stop_current_state = True\r\n            self.thread = None\r\n        self._graph = graph\r\n        self.start_node = start_node\r\n        self.current_node = self.start_node\r\n        self.set_initial_idle_state(self.planner.settings.use_all_joints)\r\n        self.planner.state_queue.reset()\r\n        self.lock.release()\r\n        if self.animation_server is not None:\r\n            #self.animation_server.start()\r\n            print(\"restarted animation server..............................\")\r\n\r\n    def create_collision_boundary(self, radius, length, visualize=True, active=True):\r\n        if not constants.activate_simulation:\r\n            return\r\n        print(\"create collision boundary\", radius, length)\r\n        self.collision_boundary = self.scene_object.scene.object_builder.create_component(\"collision_boundary\", self.scene_object, radius, length, \"morphablegraph_state_machine\", visualize=visualize)\r\n        self.collision_boundary.active = active\r\n        self.planner.collision_boundary = self.collision_boundary\r\n\r\n    def create_hand_collision_boundary(self, joint_name, radius, visualize=True, active=True):\r\n        if not constants.activate_simulation:\r\n            return\r\n        print(\"create collision boundary\",joint_name, radius)\r\n        self.hand_collision_boundary = self.scene_object.scene.object_builder.create_component(\"hand_collision_boundary\", joint_name, self.scene_object, radius, \"morphablegraph_state_machine\", visualize=visualize)\r\n        self.hand_collision_boundary.active = active\r\n        self.planner.hand_collision_boundary = self.hand_collision_boundary\r\n\r\n    def load_pfnn_controller(self, path, mean_folder, src_skeleton=None):\r\n        self.planner.pfnn_wrapper = PFNNWrapper.load_from_file(self.skeleton, path, mean_folder, src_skeleton)\r\n        self.planner.use_pfnn = True\r\n\r\n    def set_initial_idle_state(self, use_all_joints=False):\r\n        mv = MotionVector(self.skeleton)\r\n        print(\"node\", self.current_node)\r\n        mv.frames = self._graph.nodes[self.current_node].sample().get_motion_vector()\r\n        mv.frame_time = self.frame_time\r\n        mv.n_frames = len(mv.frames)\r\n        print(\"before\", mv.frames.shape, self.skeleton.reference_frame_length)\r\n        other_animated_joints = self._graph.nodes[self.current_node].get_animated_joints()\r\n        if len(other_animated_joints) == 0:\r\n            other_animated_joints = ANIMATED_JOINTS_CUSTOM\r\n        if use_all_joints:\r\n            other_animated_joints = self._graph.nodes[self.current_node].get_animated_joints()\r\n            if len(other_animated_joints) == 0:\r\n                other_animated_joints = ANIMATED_JOINTS_CUSTOM\r\n            full_frames = np.zeros((len(mv.frames), self.skeleton.reference_frame_length))\r\n            for idx, reduced_frame in enumerate(mv.frames):\r\n                full_frames[idx] = self.skeleton.add_fixed_joint_parameters_to_other_frame(reduced_frame,\r\n                                                                                           other_animated_joints)\r\n            mv.frames = full_frames\r\n        self.state = MotionState(mv)\r\n        self.state.play = self.play\r\n    \r\n    def set_config(self, config):\r\n        if \"activate_grounding\" in config:\r\n            self.activate_grounding =config[\"activate_grounding\"]\r\n        self.planner.set_config(config)\r\n\r\n    def set_visualization(self, visualization):\r\n        self._visualization = visualization\r\n        self._visualization.update_dir_vis(self.direction_vector, self.target_projection_len)\r\n        self.update_transformation()\r\n\r\n    def update(self, dt):\r\n        \"\"\" update current frame and global joint transformation matrices\r\n        \"\"\"\r\n        if self.play:\r\n            transition = self.state.update(self.speed * dt)\r\n            self.lock.acquire()\r\n            if transition or (len(self.planner.state_queue) > 0 and self.stop_current_state):\r\n                # decide if the state planner should be used based on a given task and the number of states in the queue\r\n                use_state_planner = False\r\n                #self.planner.state_queue.mutex.acquire()\r\n                if self.planner.is_processing or len(self.planner.state_queue) > 0:\r\n                    use_state_planner = True\r\n                #self.planner.state_queue.mutex.release()\r\n                if use_state_planner:\r\n                    # if the state planner should be used wait until a state was generated\r\n                    success = False\r\n                    n_queries = 0\r\n                    while not success and n_queries < self.n_max_state_queries:\r\n                        self.planner.state_queue.mutex.acquire()\r\n                        success = self.pop_motion_state_from_queue()\r\n                        if not success:\r\n                            #print(\"Warning: state queue is empty\")\r\n                            n_queries += 1\r\n                        self.planner.state_queue.mutex.release()\r\n                    if not success:\r\n                        print(\"Warning: transition to idle state due to empty state queue\")\r\n                        state_entry = self.planner.state_queue.generate_idle_state(dt, self.pose_buffer, False)\r\n                        self.set_state_entry(state_entry)\r\n                    self.stop_current_state = False\r\n                else:\r\n                    # otherwise transition to new state without the planner, e.g. to idle state\r\n                    self.transition_to_next_state_controlled()\r\n                    #print(\"WAIT\")\r\n            self.lock.release()\r\n            self.update_transformation()\r\n\r\n    def pop_motion_state_from_queue(self):\r\n        if len(self.planner.state_queue) > 0:\r\n            state_entry = self.planner.state_queue.get_first_state()\r\n            self.set_state_entry(state_entry)\r\n            self.planner.state_queue.pop_first_state()\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    def set_state_entry(self, state_entry):\r\n        self.state = state_entry.state\r\n        self.current_node = state_entry.node\r\n        self.node_type = state_entry.node_type\r\n        #print(\"set state\", self.current_node, self.state.mv.frames[:,1])\r\n        self.pose_buffer = copy.copy(state_entry.pose_buffer)\r\n\r\n    def set_global_position(self, position):\r\n        self.lock.acquire()\r\n        self.state.set_position(position)\r\n        self.set_buffer_position(position)\r\n        self.lock.release()\r\n        assert not np.isnan(self.pose_buffer[-1]).any(), \"Error in set pos \"+str(position)\r\n\r\n    def set_global_orientation(self, orientation):\r\n        self.lock.acquire()\r\n        self.state.set_orientation(orientation)\r\n        self.set_buffer_orientation(orientation)\r\n        self.lock.release()\r\n        assert not np.isnan(self.pose_buffer[-1]).any(), \"Error in set orientation \"+str(orientation)\r\n\r\n    def set_buffer_position(self, pos):\r\n        for idx in range(len(self.pose_buffer)):\r\n            self.pose_buffer[idx][:3] = pos\r\n\r\n    def set_buffer_orientation(self, orientation):\r\n        for idx in range(len(self.pose_buffer)):\r\n            self.pose_buffer[idx][3:7] = orientation\r\n        \r\n    def unpause(self):\r\n        self.state.hold_last_frame = False\r\n        self.state.paused = False\r\n\r\n    def play_clip(self, clip_name):\r\n        print(\"play clip\")\r\n        if clip_name in self.loaded_clips:\r\n            state = self.loaded_clips[clip_name]\r\n            node_id = (\"walk\", \"idle\")\r\n            node_type = NODE_TYPE_IDLE\r\n            self.lock.acquire()\r\n            self.planner.state_queue.mutex.acquire()\r\n            self.stop_current_state = True\r\n            pose_buffer = self.pose_buffer\r\n            state_entry = StateQueueEntry(node_id, node_type, state, pose_buffer)\r\n            self.set_state_entry(state_entry)\r\n            self.planner.state_queue.reset()\r\n            print(\"set state entry \", clip_name)\r\n            self.planner.state_queue.mutex.release()\r\n            self.lock.release()\r\n\r\n    def generate_action_constraints(self, action_desc):\r\n        action_name = action_desc[\"name\"]\r\n        velocity_factor = 1.0\r\n        n_cycles = 1\r\n        upper_body_gesture = None\r\n        constrain_look_at = False\r\n        look_at_constraints = False\r\n        if \"locomotionUpperBodyAction\" in action_desc:\r\n            upper_body_gesture = dict()\r\n            upper_body_gesture[\"name\"] = action_desc[\"locomotionUpperBodyAction\"]\r\n        elif \"upperBodyGesture\" in action_desc:\r\n            upper_body_gesture = action_desc[\"upperBodyGesture\"]\r\n        if \"velocityFactor\" in action_desc:\r\n            velocity_factor = action_desc[\"velocityFactor\"]\r\n        if \"nCycles\" in action_desc:\r\n            n_cycles = action_desc[\"nCycles\"]\r\n        if \"constrainLookAt\" in action_desc:\r\n            constrain_look_at = action_desc[\"constrainLookAt\"]\r\n        if \"lookAtConstraints\" in action_desc:\r\n            look_at_constraints = action_desc[\"lookAtConstraints\"]\r\n        print(\"enqueue states\", action_name)\r\n        frame_constraints, end_direction, body_orientation_targets = self.planner.constraint_builder.extract_constraints_from_dict(action_desc, look_at_constraints)\r\n        out = dict()\r\n        out[\"action_name\"] = action_name\r\n        out[\"frame_constraints\"] = frame_constraints\r\n        out[\"end_direction\"] = end_direction\r\n        out[\"body_orientation_targets\"] = body_orientation_targets\r\n        if \"controlPoints\" in action_desc:\r\n            out[\"control_points\"] = action_desc[\"controlPoints\"]\r\n        elif \"directionAngle\" in action_desc and \"nSteps\" in action_desc and \"stepDistance\" in action_desc:\r\n            root_dir = get_global_node_orientation_vector(self.skeleton, self.skeleton.aligning_root_node, self.get_current_frame(), self.skeleton.aligning_root_dir)\r\n            root_dir = np.array([root_dir[0], 0, root_dir[1]])\r\n            out[\"direction\"] = rotate_vector_deg(root_dir, action_desc[\"directionAngle\"])\r\n            out[\"n_steps\"] = action_desc[\"nSteps\"]\r\n            out[\"step_distance\"] = action_desc[\"stepDistance\"]\r\n        elif \"direction\" in action_desc and \"nSteps\" in action_desc and \"stepDistance\" in action_desc:\r\n            out[\"direction\"] = action_desc[\"direction\"]\r\n            out[\"n_steps\"] = action_desc[\"nSteps\"]\r\n            out[\"step_distance\"] = action_desc[\"stepDistance\"]\r\n        out[\"upper_body_gesture\"] = upper_body_gesture\r\n        out[\"velocity_factor\"] = velocity_factor\r\n        out[\"n_cycles\"] = n_cycles\r\n        out[\"constrain_look_at\"] = constrain_look_at\r\n        out[\"look_at_constraints\"] = look_at_constraints\r\n        return out\r\n\r\n    def enqueue_states(self, action_sequence, dt, refresh=False):\r\n        \"\"\" generates states until all control points have been reached\r\n            should to be called by extra thread to asynchronously\r\n        \"\"\"\r\n        _action_sequence = []\r\n        for action_desc in action_sequence:\r\n            \r\n            if \"collisionObjectsUpdates\" in action_desc:\r\n                func_name = \"a\"\r\n                params = self.scene_object.scene,action_desc[\"collisionObjectsUpdates\"]\r\n                self.scene_object.scene.schedule_func_call(func_name, update_collision_objects, params)\r\n            a = self.generate_action_constraints(action_desc)\r\n            _action_sequence.append(a)\r\n\r\n        if self.thread is not None:\r\n            print(\"stop thread\")\r\n            self.planner.stop_thread = True\r\n            self.thread.join()\r\n            self.stop_current_state = refresh\r\n            #self.planner.state_queue.reset()\r\n            self.thread = None\r\n\r\n        self.planner.state_queue.mutex.acquire()\r\n        start_node = self.current_node\r\n        start_node_type = self.node_type\r\n        pose_buffer = [np.array(frame) for frame in self.state.get_frames()[-self.buffer_size:]]\r\n        self.planner.state_queue.reset()\r\n        self.planner.state_queue.mutex.release()\r\n        self.planner.stop_thread = False\r\n        self.planner.is_processing = True\r\n        if refresh:\r\n            self.lock.acquire()\r\n            self.stop_current_state = True\r\n            pose_buffer = []\r\n            for p in self.pose_buffer:\r\n                pose_buffer.append(p)\r\n            #self.transition_to_next_state_controlled()\r\n            self.lock.release()\r\n\r\n        method_args = (_action_sequence, start_node, start_node_type, pose_buffer, dt)\r\n        self.thread = threading.Thread(target=self.planner.generate_motion_states_from_action_sequence, name=\"c\", args=method_args)\r\n        self.thread.start()\r\n\r\n    def draw(self, modelMatrix, viewMatrix, projectionMatrix, lightSources):\r\n        return\r\n        if self.show_skeleton:\r\n            self._visualization.draw(modelMatrix, viewMatrix, projectionMatrix, lightSources)\r\n        self._visualization.update_dir_vis(self.direction_vector, self.target_projection_len)\r\n        self.line_renderer.draw(modelMatrix, viewMatrix, projectionMatrix)\r\n\r\n\r\n    def transition_to_next_state_randomly(self):\r\n        self.current_node = self._graph.nodes[self.current_node].generate_random_transition(NODE_TYPE_STANDARD)\r\n        spline = self._graph.nodes[self.current_node].sample()\r\n        self.set_state_by_spline(spline)\r\n\r\n    def emit_update(self):\r\n        if self.activate_emit:\r\n            return\r\n            #self.update_scene_object.emit(-1)\r\n\r\n    def set_aligning_transform(self):\r\n        \"\"\" uses a random sample of the morphable model to find an aligning transformation to bring constraints into the local coordinate system\"\"\"\r\n        sample = self._graph.nodes[self.current_node].sample(False)\r\n        frames = sample.get_motion_vector()\r\n        m = get_node_aligning_2d_transform(self.skeleton, self.skeleton.aligning_root_node,\r\n                                           self.pose_buffer, frames)\r\n        self.aligning_transform = np.linalg.inv(m)\r\n\r\n    def transition_to_next_state_controlled(self):\r\n        self.current_node, self.node_type, self.node_queue = self.select_next_node(self.current_node, self.node_type, self.node_queue, self.target_projection_len)\r\n        #print(\"transition\", self.current_node, self.node_type, self.target_projection_len)\r\n        self.set_aligning_transform()\r\n        if isinstance(self._graph.nodes[self.current_node].motion_primitive, StaticMotionPrimitive):\r\n            spline = self._graph.nodes[self.current_node].sample()\r\n            new_frames = spline.get_motion_vector()\r\n        else:\r\n            mp_constraints = self.planner.constraint_builder.generate_walk_constraints(self.current_node, self.aligning_transform, self.direction_vector, self.target_projection_len, self.pose_buffer)\r\n            s = self.planner.mp_generator.generate_constrained_sample(self._graph.nodes[self.current_node], mp_constraints)\r\n            spline = self._graph.nodes[self.current_node].back_project(s, use_time_parameters=False)\r\n            new_frames = spline.get_motion_vector()\r\n            #new_frames = self.planner.generate_constrained_motion_primitive(self.current_node, mp_constraints.constraints, self.pose_buffer)\r\n        \r\n        if self.planner.settings.use_all_joints:\r\n            new_frames = self.planner.complete_frames(self.current_node, new_frames)\r\n        #new_frames = self.state.get_frames()\r\n        ignore_rotation = False\r\n        if self.current_node[1] == \"idle\" and self.planner.settings.ignore_idle_rotation:\r\n            ignore_rotation = True\r\n        self.state = self.planner.state_queue.build_state(new_frames, self.pose_buffer, ignore_rotation)\r\n        self.state.play = self.play\r\n        self.emit_update()\r\n\r\n    def select_next_node(self, current_node, current_node_type, node_queue, step_distance):\r\n        if len(node_queue):\r\n            next_node, node_type = node_queue[0]\r\n            node_queue = node_queue[1:]\r\n            next_node_type = node_type\r\n        else:\r\n            next_node_type = self.planner.get_next_node_type(current_node_type, step_distance)\r\n            next_node = self._graph.nodes[current_node].generate_random_transition(next_node_type)\r\n            if next_node is None:\r\n               next_node = self.start_node\r\n               next_node_type = NODE_TYPE_IDLE\r\n        return next_node, next_node_type, node_queue\r\n\r\n    def apply_ik_on_transition(self, spline):\r\n        left_foot = self.skeleton.skeleton_model[\"joints\"][\"left_foot\"]\r\n        right_foot = self.skeleton.skeleton_model[\"joints\"][\"right_foot\"]\r\n        right_hand = self.skeleton.skeleton_model[\"joints\"][\"right_wrist\"]\r\n        left_hand = self.skeleton.skeleton_model[\"joints\"][\"left_wrist\"]\r\n        n_coeffs = len(spline.coeffs)\r\n        ik_chains = self.skeleton.skeleton_model[\"ik_chains\"]\r\n        ik_window = 5  # n_coeffs - 2\r\n        align_joint(self.skeleton, spline.coeffs, 0, left_foot, ik_chains[\"foot_l\"], ik_window)\r\n        align_joint(self.skeleton, spline.coeffs, 0, right_foot, ik_chains[\"foot_r\"], ik_window)\r\n        align_joint(self.skeleton, spline.coeffs, 0, left_hand, ik_chains[left_hand], ik_window)\r\n        align_joint(self.skeleton, spline.coeffs, 0, right_hand, ik_chains[right_hand], ik_window)\r\n\r\n        for i in range(1, n_coeffs):\r\n            spline.coeffs[i] = self.align_frames(spline.coeffs[i], spline.coeffs[0])\r\n\r\n    def align_frames(self, frame, ref_frame):\r\n        for i in range(self.n_joints):\r\n            o = i*4+3\r\n            q = frame[o:o+4]\r\n            frame[o:o+4] = -q if np.dot(ref_frame[o:o+4], q) < 0 else q\r\n        return frame\r\n\r\n    def update_transformation(self):\r\n        pose = self.state.get_pose()\r\n        if self.activate_grounding:\r\n            pose = self.motion_grounding.apply_on_frame(pose, self.scene_object.scene)\r\n        self.pose_buffer.append(np.array(pose))\r\n        _pose = copy.copy(pose)\r\n        #_pose[:3] = [0,0,0]\r\n        if self.show_skeleton and self._visualization is not None:\r\n            self._visualization.updateTransformation(_pose , self.scene_object.scale_matrix)\r\n            self._visualization.update_dir_vis(self.direction_vector, self.target_projection_len)\r\n        self.pose_buffer = self.pose_buffer[-self.buffer_size:]\r\n        if self.is_recording:\r\n            self.recorded_poses.append(pose)\r\n\r\n    def getPosition(self):\r\n        if self.state is not None:\r\n            return self.state.get_pose()[:3]\r\n        else:\r\n            return [0, 0, 0]\r\n\r\n    def get_global_transformation(self):\r\n        return self.skeleton.nodes[self.skeleton.root].get_global_matrix(self.pose_buffer[-1])\r\n\r\n    def handle_keyboard_input(self, key):\r\n        if key == \"p\":\r\n            self.transition_to_action(\"placeLeft\")\r\n        else:\r\n            if key == \"a\":\r\n                self.rotate_dir_vector(-10)\r\n            elif key == \"d\":\r\n                self.rotate_dir_vector(10)\r\n            elif key == \"w\":\r\n                self.target_projection_len += 10\r\n                self.target_projection_len = min(self.target_projection_len, self.max_step_length)\r\n            elif key == \"s\":\r\n                self.target_projection_len -= 10\r\n                self.target_projection_len = max(self.target_projection_len, 0)\r\n            #if self.node_type == NODE_TYPE_IDLE:\r\n            #    self.transition_to_next_state_controlled()\r\n            #if not self.play and self.node_type == NODE_TYPE_END and self.target_projection_len > 0:\r\n            #    self.play = True\r\n        self.emit_update()\r\n    \r\n    def create_action_constraint(self, action_name, keyframe_label, position, joint_name=None):\r\n        node = self.actions[action_name][\"constraint_slots\"][keyframe_label][\"node\"]\r\n        if joint_name is None:\r\n            joint_name = self.actions[action_name][\"constraint_slots\"][keyframe_label][\"joint\"]\r\n        action_constraint = UnityFrameConstraint((action_name, node), keyframe_label, joint_name, position, None)\r\n        return action_constraint\r\n\r\n    def transition_to_action(self, action, constraint=None):\r\n        self.action_constraint  = constraint\r\n        if self.current_node[0] != \"walk\":\r\n            return\r\n        for node_name, node_type in self.actions[action][\"node_sequence\"]:\r\n            self.node_queue.append(((action, node_name), node_type))\r\n        if self.node_type == NODE_TYPE_IDLE:\r\n            self.node_queue.append((self.start_node, NODE_TYPE_IDLE))\r\n        self.transition_to_next_state_controlled()\r\n\r\n    def rotate_dir_vector(self, angle):\r\n        r = np.radians(angle)\r\n        s = np.sin(r)\r\n        c = np.cos(r)\r\n        self.direction_vector[0] = c * self.direction_vector[0] - s * self.direction_vector[2]\r\n        self.direction_vector[2] = s * self.direction_vector[0] + c * self.direction_vector[2]\r\n        self.direction_vector /= np.linalg.norm(self.direction_vector)\r\n        print(\"rotate\",self.direction_vector)\r\n\r\n    def get_n_frames(self):\r\n        return self.state.get_n_frames()\r\n\r\n    def get_frame_time(self):\r\n        return self.state.get_frame_time()\r\n\r\n    def get_pose(self, frame_idx=None):\r\n        frame = self.state.get_pose(frame_idx)\r\n        if self.retarget_engine is not None:\r\n            return self.retarget_engine.retarget_frame(frame, None)\r\n        else:\r\n            return frame\r\n        \r\n\r\n    def get_current_frame_idx(self):\r\n        return self.state.frame_idx\r\n\r\n    def get_current_annotation(self):\r\n        return self.state.get_current_annotation()\r\n\r\n    def get_n_annotations(self):\r\n        return self.state.get_n_annotations()\r\n\r\n    def get_semantic_annotation(self):\r\n        return self.state.get_semantic_annotation()\r\n\r\n    def set_target_skeleton(self, target_skeleton):\r\n        self.target_skeleton = target_skeleton\r\n        target_knee = target_skeleton.skeleton_model[\"joints\"][\"right_knee\"]\r\n        src_knee = self.skeleton.skeleton_model[\"joints\"][\"right_knee\"]\r\n        scale = 1.0 # np.linalg.norm(target_skeleton.nodes[target_knee].offset) / np.linalg.norm(self.skeleton.nodes[src_knee].offset)\r\n        joint_map = generate_joint_map(self.skeleton.skeleton_model, target_skeleton.skeleton_model)\r\n        skeleton_copy = copy.deepcopy(self.skeleton)\r\n        self.retarget_engine = Retargeting(skeleton_copy, target_skeleton, joint_map, scale, additional_rotation_map=None, place_on_ground=False)\r\n        self.activate_emit = False\r\n        self.show_skeleton = False\r\n\r\n    def get_actions(self):\r\n        return list(self.actions.keys())\r\n\r\n    def get_keyframe_labels(self, action_name):\r\n        if action_name in self.actions:\r\n            if \"constraint_slots\" in self.actions[action_name]:\r\n                return list(self.actions[action_name][\"constraint_slots\"].keys())\r\n            else:\r\n                raise Exception(\"someting \"+ action_name)\r\n        return list()\r\n\r\n    def get_skeleton(self):\r\n        if self.target_skeleton is not None:\r\n            return self.target_skeleton\r\n        else:\r\n            return self.skeleton\r\n\r\n    def get_animated_joints(self):\r\n        return self._graph.animated_joints\r\n\r\n    def get_current_frame(self):\r\n        pose = self.state.get_pose(None)\r\n        if self.target_skeleton is not None:\r\n            pose = self.retarget_engine.retarget_frame(pose, self.target_skeleton.reference_frame)\r\n            if self.activate_grounding:\r\n                x = pose[0]\r\n                z = pose[2]\r\n                target_ground_height = self.scene_object.scene.get_height(x, z)\r\n                #pelvis = self.target_skeleton.skeleton_model[\"joints\"][\"pelvis\"]\r\n                #offset = self.target_skeleton.nodes[pelvis].offset\r\n                #print(\"offset\", pelvis, offset[2],np.linalg.norm(offset))\r\n                #shift = target_ground_height - (pose[1] + offset[2])\r\n                shift = target_ground_height - pose[1]\r\n                pose[1] += shift\r\n        return pose\r\n\r\n    def get_events(self):\r\n        event_keys = list(self.state.events.keys())\r\n        for key in event_keys:\r\n            if self.state.frame_idx >= key:\r\n                # send and remove event\r\n                events = self.state.events[key]\r\n                del self.state.events[key]\r\n                return events\r\n        else:\r\n            return list()\r\n\r\n    def get_current_annotation_label(self):\r\n        return \"\"\r\n\r\n    def isPlaying(self):\r\n        return True\r\n\r\n    def has_success(self):\r\n        return self.success\r\n\r\n    def reset_planner(self):\r\n        print(\"reset planner\")\r\n        self.planner.state_queue.mutex.acquire()\r\n        if self.planner.is_processing:\r\n            self.planner.stop_thread = True\r\n            if self.thread is not None:\r\n                self.thread.stop()\r\n            self.planner.is_processing = False\r\n            #self.current_node = (\"walk\", \"idle\")\r\n            self.current_node = self.start_node\r\n            self.node_type = NODE_TYPE_IDLE\r\n            self.planner.state_queue.reset()\r\n            self.pose_buffer = list()\r\n            self.set_initial_idle_state(self.use_all_joints)\r\n\r\n        self.planner.state_queue.mutex.release()\r\n        return\r\n\r\n    def start_recording(self):\r\n        self.is_recording = True\r\n        self.recorded_poses = list()\r\n\r\n    def save_recording_to_file(self):\r\n        time_str = datetime.now().strftime(\"%d%m%y_%H%M%S\")\r\n        filename = \"recording_\"+time_str+\".bvh\"\r\n        n_frames = len(self.recorded_poses)\r\n        if n_frames > 0:\r\n            other_animated_joints = self._graph.nodes[self.current_node].get_animated_joints()\r\n            full_frames = np.zeros((n_frames, self.skeleton.reference_frame_length))\r\n            for idx, reduced_frame in enumerate(self.recorded_poses):\r\n                full_frames[idx] = self.skeleton.add_fixed_joint_parameters_to_other_frame(reduced_frame,\r\n                                                                                           other_animated_joints)\r\n            mv = MotionVector()\r\n            mv.frames = full_frames\r\n            mv.n_frames = n_frames\r\n            mv.frame_time = self.frame_time\r\n            mv.export(self.skeleton, filename)\r\n            print(\"saved recording with\", n_frames, \"to file\", filename)\r\n            self.is_recording = False\r\n\r\n    def get_bone_matrices(self):\r\n        return self._visualization.matrices\r\n\r\n    def handle_collision(self):\r\n        print(\"handle collision\")\r\n        self.lock.acquire()\r\n        if self.thread is not None:\r\n            print(\"stop thread\")\r\n            self.planner.stop_thread = True\r\n            self.thread.join()\r\n            self.stop_current_state = True\r\n            self.thread = None\r\n\r\n        self.planner.state_queue.mutex.acquire()\r\n        self.planner.state_queue.reset()\r\n        self.planner.state_queue.mutex.release()\r\n        self.planner.stop_thread = False\r\n        self.planner.is_processing = True\r\n        \r\n        self.stop_current_state = True\r\n        #self.transition_to_next_state_controlled()\r\n        self.lock.release()\r\n    \r\ndef load_morphable_graph_state_machine(builder, path, use_all_joints=True):\r\n    scene_object = SceneObject()\r\n    scene_object.scene = builder._scene\r\n    builder.create_component(\"morphablegraph_state_machine\", scene_object, path, use_all_joints)\r\n    builder._scene.addObject(scene_object)\r\n    return scene_object\r\n\r\ndef load_morphable_graph_state_machine_from_db(builder, db_path, skeleton_name, graph_id, use_all_joints=False, config=DEFAULT_CONFIG):\r\n    scene_object = SceneObject()\r\n    scene_object.scene = builder._scene\r\n    builder.create_component(\"morphablegraph_state_machine_from_db\", scene_object,  db_path, skeleton_name, graph_id, use_all_joints, config)\r\n    builder._scene.addObject(scene_object)\r\n    return scene_object\r\n\r\ndef attach_mg_state_machine(builder, scene_object,file_path, use_all_joints=True, config=DEFAULT_CONFIG):\r\n    color=get_random_color()  \r\n    loader = MotionStateGraphLoader()\r\n    loader.use_all_joints = use_all_joints# = set animated joints to all\r\n    if os.path.isfile(file_path):\r\n        loader.set_data_source(file_path[:-4])\r\n        graph = loader.build()\r\n        name = file_path.split(\"/\")[-1]\r\n        start_node = None\r\n        animation_controller = MorphableGraphStateMachine(scene_object, graph, start_node, use_all_joints=use_all_joints, config=config, pfnn_data=loader.pfnn_data)\r\n        scene_object.add_component(\"morphablegraph_state_machine\", animation_controller)\r\n        scene_object.name = name\r\n        if builder._scene.visualize:\r\n            vis = builder.create_component(\"skeleton_vis\", scene_object, animation_controller.get_skeleton(), color)\r\n            animation_controller.set_visualization(vis)\r\n            #scene_object._components[\"morphablegraph_state_machine\"].update_scene_object.connect(builder._scene.slotUpdateSceneObjectRelay)\r\n\r\n        agent = SimpleNavigationAgent(scene_object)\r\n        scene_object.add_component(\"nav_agent\", agent)\r\n        return animation_controller\r\n\r\ndef attach_mg_state_machine_from_db(builder, scene_object, db_url, skeleton_name, graph_id, use_all_joints=False, config=DEFAULT_CONFIG):\r\n    color=get_random_color()\r\n    loader = MotionStateGraphLoader()\r\n    # set animated joints to all necessary for combination of models with different joints\r\n    loader.use_all_joints = use_all_joints\r\n    frame_time = 1.0/72\r\n    graph = loader.build_from_database(db_url, skeleton_name, graph_id, frame_time)\r\n    start_node = None\r\n    name = skeleton_name\r\n    animation_controller = MorphableGraphStateMachine(scene_object, graph, start_node, use_all_joints=use_all_joints, config=config, pfnn_data=loader.pfnn_data)\r\n    scene_object.add_component(\"morphablegraph_state_machine\", animation_controller)\r\n    scene_object.name = name\r\n    if builder._scene.visualize:\r\n        vis = builder.create_component(\"skeleton_vis\", scene_object, animation_controller.get_skeleton(), color)\r\n        animation_controller.set_visualization(vis)\r\n        #scene_object._components[\"morphablegraph_state_machine\"].update_scene_object.connect(builder._scene.slotUpdateSceneObjectRelay)\r\n\r\n    agent = SimpleNavigationAgent(scene_object)\r\n    scene_object.add_component(\"nav_agent\", agent)\r\n    return animation_controller\r\n\r\n\r\nSceneObjectBuilder.register_component(\"morphablegraph_state_machine\", attach_mg_state_machine)\r\nSceneObjectBuilder.register_component(\"morphablegraph_state_machine_from_db\", attach_mg_state_machine_from_db)\r\n\r\nSceneObjectBuilder.register_file_handler(\"mg.zip\", load_morphable_graph_state_machine)\r\nSceneObjectBuilder.register_object(\"mg_state_machine_from_db\", load_morphable_graph_state_machine_from_db)\r\ntry:\r\n    from functools import partial\r\n    from tool.core.editor_window import EditorWindow, open_file_dialog\r\n    mg_menu_actions = [{\"text\": \"Load MG State Machine\", \"function\": partial(open_file_dialog, \"mg.zip\")}]\r\n    EditorWindow.add_actions_to_menu(\"File\", mg_menu_actions)\r\nexcept:\r\n    pass",
            "requirements": "pip install git+https://github.com/dfki-asr/morphablegraphs"
        },
        "mpm:vis_utils": {
            "dataType": "mpm",
            "engine": "vis_utils",
            "script": "#!/usr/bin/env python\n#\n# Copyright 2019 DFKI GmbH.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the\n# following conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n# NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n# USE OR OTHER DEALINGS IN THE SOFTWARE.\nimport json\nimport bson\nimport bz2\nimport numpy as np\nfrom copy import deepcopy\nimport collections\nfrom vis_utils.animation.animation_controller import CONTROLLER_TYPE_MP\nfrom vis_utils.animation.skeleton_animation_controller import LegacySkeletonAnimationController\nfrom vis_utils.animation.skeleton_visualization import SkeletonVisualization, SKELETON_DRAW_MODE_LINES\nfrom anim_utils.animation_data import BVHWriter, MotionVector, SkeletonBuilder\nfrom anim_utils.utilities.io_helper_functions import load_json_file, write_to_json_file\nfrom anim_utils.utilities.log import set_log_mode, LOG_MODE_DEBUG\nfrom vis_utils.scene.utils import get_random_color\nfrom vis_utils.scene.scene_object_builder import SceneObjectBuilder, SceneObject\nfrom morphablegraphs.motion_model.motion_state_graph_node import MotionStateGraphNode\nfrom morphablegraphs.motion_generator.motion_primitive_generator import MotionPrimitiveGenerator\nfrom morphablegraphs.constraints.motion_primitive_constraints import MotionPrimitiveConstraints\nfrom morphablegraphs.constraints.action_constraints import ActionConstraints\nfrom morphablegraphs import MotionGenerator, GraphWalkOptimizer, AnnotatedMotionVector, DEFAULT_ALGORITHM_CONFIG\nfrom morphablegraphs.constraints.spatial_constraints import GlobalTransformConstraint\nfrom morphablegraphs.motion_generator.optimization.optimizer_builder import OptimizerBuilder\nfrom morphablegraphs.space_partitioning import FeatureClusterTree\nfrom morphablegraphs.utilities import convert_to_mgrd_skeleton\n\nclass MockGraph(object):\n    def __init__(self, skeleton):\n        self.skeleton = skeleton\n\n\nclass MotionPrimitiveController(LegacySkeletonAnimationController):\n    def __init__(self, scene_object, name, data, color=(0, 0, 1)):\n        LegacySkeletonAnimationController.__init__(self, scene_object)\n        self.motion_primitive = MotionStateGraphNode(None)\n        self.skeleton = SkeletonBuilder().load_from_json_data(data[\"skeleton\"])\n        self.frameTime = self.skeleton.frame_time\n        self._visualizations = []\n        self.samples = []\n        self.algorithm_config = DEFAULT_ALGORITHM_CONFIG\n        self.color = color\n        self.motion_primitive._initialize_from_json(convert_to_mgrd_skeleton(self.skeleton), data)\n        print(\"loaded motion primitive\")\n        print(\"spatial\", self.motion_primitive.get_n_spatial_components())\n        print(\"time\", self.motion_primitive.get_n_time_components())\n        self.motion_primitive.cluster_tree = None\n\n        self.training_data = None\n        #print(\"n gmm\", len(self.motion_primitive.get_gaussian_mixture_model().weights))\n\n        self.name = name\n        self._regenerate = True\n        self.type = CONTROLLER_TYPE_MP\n        set_log_mode(LOG_MODE_DEBUG)\n        self.start_pose = {\"position\": [0, 0, 0], \"orientation\": [0, 0, 0]}\n        self.mock_graph = MockGraph(self.skeleton)\n        self.label_color_map = dict()\n\n    def init_visualization(self):\n        self.generate_random_samples(1)\n\n    def load_cluster_tree_from_json_file(self, filepath):\n        tree_data = load_json_file(filepath)\n        self.load_cluster_tree_from_json(tree_data)\n\n    def load_cluster_tree_from_json(self, tree_data):\n        print(\"load cluster tree\")\n        self.motion_primitive.cluster_tree = FeatureClusterTree.load_from_json(tree_data)\n        print(\"finished loading cluster tree\")\n\n    def clear(self):\n        self.samples = []\n        self._visualizations = []\n\n    def generate_random_samples(self, n_samples, sample_offset=0):\n        self.clear()\n        if n_samples > 1:\n            x_offset = -n_samples / 2 * sample_offset\n        else:\n            x_offset = 0\n        for idx in range(n_samples):\n            self.generate_random_sample(x_offset)\n            x_offset += sample_offset\n        self.updated_frame()\n\n    def generate_random_samples_from_tree(self, n_samples, sample_offset=0):\n        if self.motion_primitive.cluster_tree is None:\n            return\n        self.clear()\n        if n_samples > 1:\n            x_offset = -n_samples / 2 * sample_offset\n        else:\n            x_offset = 0\n        for idx in range(n_samples):\n            self.generate_random_sample_from_tree(x_offset)\n            x_offset += sample_offset\n        self.updated_frame()\n\n    def generate_random_constraints(self, joint_name, frame_idx, n_samples):\n        positions = []\n        for idx in range(n_samples):\n            positions.append(self.generate_random_constraint(joint_name, frame_idx))\n        return positions\n\n    def generate_random_sample(self, x_offset=0):\n        spline = self.motion_primitive.sample(use_time=False)\n        frames = spline.get_motion_vector()\n        self.create_sample_visualization(frames, x_offset)\n\n    def generate_random_sample_from_tree(self, x_offset=0):\n        if self.motion_primitive.cluster_tree is not None:\n            n_points = len(self.motion_primitive.cluster_tree.data)\n            sample_idx = np.random.randint(0, n_points)\n            print(\"visualize\", sample_idx, \"/\", n_points)\n            sample = self.motion_primitive.cluster_tree.data[sample_idx]\n            frames = self.motion_primitive.back_project(sample, False).get_motion_vector()\n            self.create_sample_visualization(frames, x_offset)\n\n\n    def generate_random_sample_from_data(self, x_offset=0):\n        self.clear()\n        if self.training_data is not None:\n            n_samples = len(self.training_data)\n            sample_idx = np.random.randint(0, n_samples)\n            print(\"visualize\", sample_idx, \"/\", n_samples)\n            sample = self.training_data[sample_idx]\n            frames = self.motion_primitive.back_project(sample, False).get_motion_vector()\n            self.create_sample_visualization(frames, x_offset)\n\n\n    def create_sample_visualization(self, frames, x_offset=0):\n        motion = AnnotatedMotionVector(skeleton=self.skeleton)\n        frames[:, 0] += x_offset\n        print(frames.shape)\n        motion.append_frames(frames)\n        v = SkeletonVisualization(self.scene_object, self.color)\n        v.set_skeleton(self.skeleton)\n        v.draw_mode = SKELETON_DRAW_MODE_LINES\n        self._visualizations.append(v)\n        self.samples.append(motion)\n\n    def generate_constrained_sample(self, joint_name, frame_idx, position):\n        action_constraints = ElementaryActionConstraints()\n        action_constraints.motion_state_graph = self.mock_graph\n        self.algorithm_config[\"local_optimization_settings\"][\"max_iterations\"] = 50000\n        self.algorithm_config[\"local_optimization_settings\"][\"method\"] = \"L-BFGS-B\"\n        mp_generator = MotionPrimitiveGenerator(action_constraints, self.algorithm_config)\n        #mp_generator.numerical_minimizer = OptimizerBuilder(self.algorithm_config).build_path_following_minimizer()\n        mp_generator.numerical_minimizer = OptimizerBuilder(self.algorithm_config).build_path_following_with_likelihood_minimizer()\n\n        mp_constraints = MotionPrimitiveConstraints()\n        n_frames = self.getNumberOfFrames()\n        if frame_idx == -1:\n            frame_idx = n_frames-1\n        mp_constraints.skeleton = self.skeleton\n\n        c_desc = {\"joint\": joint_name, \"canonical_keyframe\": frame_idx, \"position\": position, \"n_canonical_frames\": n_frames, \"semanticAnnotation\": {\"keyframeLabel\":\"none\"}}\n        print(\"set constraint\", c_desc)\n        c = GlobalTransformConstraint(self.skeleton, c_desc, 1.0, 1.0)\n        mp_constraints.constraints.append(c)\n        mp_constraints.use_local_optimization = self.algorithm_config[\"local_optimization_mode\"] in [\"all\", \"keyframes\"]\n        vector = mp_generator.generate_constrained_sample(self.motion_primitive, mp_constraints)\n        spline = self.motion_primitive.back_project(vector, use_time_parameters=False)\n\n        frames = spline.get_motion_vector()\n        self.create_sample_visualization(frames)\n\n    def generate_random_constraint(self, joint_name, frame_idx):\n        spline = self.motion_primitive.sample(use_time=False)\n        frames = spline.get_motion_vector()\n        position = self.skeleton.nodes[joint_name].get_global_position(frames[frame_idx])\n        return position\n\n    def updated_frame(self):\n        prevPlayAnimation = self.playAnimation\n        self.playAnimation = True\n        self.update(0)\n        self.playAnimation = prevPlayAnimation\n\n    def getNumberOfFrames(self):\n        if self.isLoadedCorrectly():\n            return len(self.samples[0].frames)\n        else:\n            return 0\n\n    def isLoadedCorrectly(self):\n        return len(self.samples) > 0\n\n    def export_to_file(self, filename, sample_idx=0):\n        if sample_idx < len(self.samples):\n            frame_time = self.frameTime\n            frames = self.skeleton.add_fixed_joint_parameters_to_motion(self.samples[0].frames)\n            bvh_writer = BVHWriter(None, self.skeleton, frames, frame_time, True)\n            bvh_writer.write(filename)\n\n    def get_skeleton_copy(self):\n        skeleton = deepcopy(self.skeleton)\n        count = 0\n        for node_key in skeleton.get_joint_names():\n            if node_key != skeleton.root:\n                skeleton.nodes[node_key].quaternion_frame_index = count\n            count += 1\n        return skeleton\n\n    def get_motion_vector_copy(self, start_frame, end_frame, sample_idx=0):\n        if sample_idx < len(self.samples):\n            mv_copy = MotionVector()\n            mv_copy.frames = deepcopy(self.samples[0].frames[start_frame:end_frame])\n            mv_copy.frames = self.skeleton.add_fixed_joint_parameters_to_motion(mv_copy.frames)\n            mv_copy.n_frames = len(mv_copy.frames)\n            mv_copy.frame_time = self.frameTime\n            return mv_copy\n\n    def draw(self, modelMatrix, viewMatrix, projectionMatrix, lightSources):\n        if self.isLoadedCorrectly() and 0 <= self.currentFrameNumber < self.getNumberOfFrames():\n                for v in self._visualizations:\n                    v.draw(modelMatrix, viewMatrix, projectionMatrix, lightSources)\n\n    def updateTransformation(self):\n        if self.isLoadedCorrectly() and 0 <= self.currentFrameNumber < self.getNumberOfFrames():\n            # update global transformation matrices of joints\n            for idx, motion in enumerate(self.samples):\n                current_frame = motion.frames[self.currentFrameNumber]\n                self._visualizations[idx].updateTransformation(current_frame, self.scene_object.scale_matrix)\n\n    def setColor(self, color):\n        print(\"set color\", color)\n        self.color = color\n        for v in self._visualizations:\n            v.set_color(color)\n\n    def create_blend_controller(self):\n        skeleton = self.skeleton\n        motions = self.samples\n        name = \"Blend Controller\" + self.name\n        self.scene_object.scene.object_builder.create_blend_controller(name, skeleton, motions)\n\n    def update_markers(self):\n        return\n\n    def set_config(self, algorithm_config):\n        self.algorithm_config = algorithm_config\n\n    def getFrameTime(self):\n        return self.frameTime\n\n    def get_semantic_annotation(self):\n        n_keys = len(self.motion_primitive.keyframes)\n        if n_keys <= 0:\n            return None\n        else:\n            sorted_keyframes = collections.OrderedDict(sorted(self.motion_primitive.keyframes.items(), key=lambda t: t[1]))\n            start = 0\n            end = int(self.motion_primitive.get_n_canonical_frames())\n            semantic_annotation = collections.OrderedDict()\n            for k, v in sorted_keyframes.items():\n                semantic_annotation[k] = list(range(start,  v))\n                self.label_color_map[k] = get_random_color()\n                start = v\n            k = \"contact\"+str(n_keys)\n            semantic_annotation[k] = list(range(start, end))\n            self.label_color_map[k] = get_random_color()\n            return list(semantic_annotation.items())\n\n    def get_label_color_map(self):\n        return self.label_color_map\n\n    def set_frame_time(self, frame_time):\n        self.frameTime = frame_time\n\n    def get_frame_time(self):\n        return self.frameTime\n\ndef load_motion_primitive(builder, file_path):\n    scene_object = SceneObject()\n    with open(file_path, \"rb\") as in_file:\n        data = in_file.read()\n        data = bz2.decompress(data)\n        data = bson.loads(data)\n    name = file_path.split(\"/\")[-1]\n    animation_controller = MotionPrimitiveController(scene_object, name, data, color=get_random_color())\n    scene_object.add_component(\"motion_primitive_controller\", animation_controller)\n    scene_object.name = animation_controller.name\n    animation_controller.init_visualization()\n    builder._scene.addAnimationController(scene_object, \"motion_primitive_controller\")\n    return scene_object\n    \ndef create_motion_primitive(builder, name, skeleton_data, data, cluster_tree_data=None):\n    scene_object = SceneObject()\n    \n    data = bz2.decompress(data)\n    data = bson.loads(data)\n    animation_controller = MotionPrimitiveController(scene_object, name, data, color=get_random_color())\n    if cluster_tree_data is not None:\n        animation_controller.load_cluster_tree_from_json(cluster_tree_data)\n    scene_object.add_component(\"motion_primitive_controller\", animation_controller)\n    scene_object.name = animation_controller.name\n    animation_controller.init_visualization()\n    builder._scene.addAnimationController(scene_object, \"motion_primitive_controller\")\n    return scene_object\n\nSceneObjectBuilder.register_object(\"mpm\", create_motion_primitive)\nSceneObjectBuilder.register_file_handler(\"mpm\", load_motion_primitive)\ntry:\n    from functools import partial\n    from tool.core.editor_window import EditorWindow, open_file_dialog\n    mg_menu_actions = [{\"text\": \"Load Morphable Model\", \"function\": partial(open_file_dialog, \"mm.json\")}]\n\n    EditorWindow.add_actions_to_menu(\"File\", mg_menu_actions)\nexcept:\n    pass",
            "requirements": "pip install git+https://github.com/dfki-asr/morphablegraphs"
        },
        "mm:vis_utils": {
            "dataType": "mm",
            "engine": "vis_utils",
            "script": "import numpy as np\nimport os\nfrom PySignal import Signal\nfrom motion_matching.mm_database import MMDatabase\nfrom motion_matching.mm_database_binary_io import MMDatabaseBinaryIO\nfrom motion_matching.mm_controller import MMController\nfrom motion_matching.mm_batch_controller import MMBatchController\nfrom vis_utils.scene.scene_object_builder import SceneObject, SceneObjectBuilder\nfrom vis_utils.scene.components import ComponentBase\nfrom vis_utils.animation.animation_controller import AnimationController\nfrom vis_utils.graphics.renderer import SphereRenderer\nfrom vis_utils.graphics.renderer.lines import DebugLineRenderer\nfrom vis_utils.graphics.utils import get_translation_matrix\nfrom vis_utils.scene.utils import get_random_color\nfrom transformations import quaternion_matrix\n\nDEFAULT_COLOR = [0, 0, 1]\n\n\nclass MMControllerVisualization(ComponentBase, AnimationController):\n    updated_animation_frame = Signal()\n    reached_end_of_animation = Signal()\n    def __init__(self, scene_object, color=DEFAULT_COLOR, visualize=True, vis_scale=0.1, use_batch=False):\n        ComponentBase.__init__(self, scene_object)\n        AnimationController.__init__(self)        \n        self.visualize = visualize\n        self.skeleton = None\n        if visualize:\n            self._sphere = SphereRenderer(10, 10, vis_scale, color=color)\n            a = [0, 0, 0]\n            b = [1, 0, 0]\n            self._line = DebugLineRenderer(a, b, [0, 1, 0])\n            self._vel_line = DebugLineRenderer(a, b, [1, 0, 0])\n            self._ref_vel_line = DebugLineRenderer(a, b, [0, 0,1])\n        self.debug_line_len = 5\n        if use_batch:\n            self.mm_controller = MMBatchController(10, [10,0,10])\n        else:\n            self.mm_controller = MMController()\n        self.use_batch = use_batch\n        self.enable_velocity_visualization = True\n        self.velocity_scale = 1\n        self.figure_c = None\n\n    def toggle_animation_loop(self):\n        self.loopAnimation = not self.loopAnimation\n\n    def isLoadedCorrectly(self):\n        return self.mm_controller.mm_database is not None\n\n    def set_data(self, mm_database, scale=1.0):\n        self.mm_controller.set_data(mm_database, scale)\n\n    def set_animator_target(self, target_controller, src_skeleton):\n        self.mm_controller.set_animator_target(target_controller, src_skeleton)\n\n    def update_debug_vis(self):\n        pos = np.array(self.mm_controller.pose.sim_position[:])\n        pos[1] = 5\n        target = pos + self.mm_controller.target_dir * self.mm_controller.target_velocity* self.debug_line_len\n        self._line.set_line(pos, target)\n\n\n        target = pos + self.mm_controller.pose.get_linear_velocity() * self.debug_line_len\n        self._vel_line.set_line(pos, target)\n\n        pos = np.array(self.mm_controller.pose.ref_position[:])\n        target = pos + self.mm_controller.pose.get_ref_linear_velocity() * self.debug_line_len\n        self._ref_vel_line.set_line(pos, target)\n        return\n\n    def update(self, dt):\n        self.velocity_scale  = dt\n        self.update_debug_vis()\n        if not self.isLoadedCorrectly() or not self.playAnimation:\n            return\n        self.currentFrameNumber = self.mm_controller.update(dt* self.animationSpeed)\n        \n        self.animationTime += dt* self.animationSpeed\n        self.mm_controller.update_target()\n        # update gui\n        if self.currentFrameNumber > self.getNumberOfFrames():\n            self.resetAnimationTime()\n        else:\n            self.updated_animation_frame.emit(self.currentFrameNumber)\n\n    def draw(self, model, view, projection, lights):\n        if self._sphere is None:\n                return\n        if self.use_batch:\n            for pose in self.mm_controller.env_poses:\n                self.draw_pose(pose, model, view, projection, lights)\n        else:\n            self.draw_pose(self.mm_controller.pose,  model, view, projection, lights)\n        self._line.draw(model, view, projection)\n        self._vel_line.draw(model, view, projection)\n        self._ref_vel_line.draw(model, view, projection)\n        if self.enable_velocity_visualization:\n            self.draw_velocity(model, view, projection)\n            if self.figure_c is not None:\n                self.draw_figure_velocity(model, view, projection)\n\n    def draw_pose(self, pose, m, v, p, lights):#\n        for position in pose.global_positions:\n            tm = get_translation_matrix(position[:3])\n            self._sphere.draw(np.dot(tm, m), v, p, lights)\n        return\n\n    def draw_velocity(self, m, v, p):\n        root_m = quaternion_matrix(self.mm_controller.pose.sim_rotation)[:3,:3]\n        frame = self.mm_controller.mm_database.get_relative_bone_velocities_frame(self.currentFrameNumber)\n        for i, position in  enumerate(self.mm_controller.pose.global_positions):\n            lv = frame[i]\n            lv = np.dot(root_m, lv)\n            #print(self.mm_controller.mm_database.bone_names[i], lv)\n            self._ref_vel_line.set_line(position, position-lv*self.velocity_scale)\n            self._ref_vel_line.draw(m, v, p)\n\n    def draw_figure_velocity(self, m, v, p):\n        for b in self.figure_c.target_figure.bodies:\n            position = np.array(self.figure_c.target_figure.bodies[b].get_position())\n            lv = np.array(self.figure_c.target_figure.bodies[b].get_linear_velocity())\n            self._vel_line.set_line(position, position-lv*self.velocity_scale)\n            self._vel_line.draw(m, v, p)\n\n\n    def getNumberOfFrames(self):\n        return self.mm_controller.n_frames\n\n    def getFrameTime(self):\n        return self.mm_controller.frame_time\n\n    def updateTransformation(self, frame_number=None):\n        if frame_number is not None:\n            self.currentFrameNumber = frame_number\n        self.animationTime = self.getFrameTime() * self.currentFrameNumber\n\n    def setCurrentFrameNumber(self, frame_number=None):\n        if frame_number is not None:\n            self.currentFrameNumber = frame_number\n        self.animationTime = self.getFrameTime() * self.currentFrameNumber\n\n\n    def setColor(self, color):\n        self._sphere.technique.material.diffuse_color = color\n        self._sphere.technique.material.ambient_color = color * 0.1\n\n    def getColor(self):\n        return self._sphere.technique.material.diffuse_color\n\n\n    def get_current_frame(self):\n        return self.pose.fk_positions\n\n    def get_skeleton(self):\n        return self.skeleton\n\n    def get_frame_time(self):\n        return self.getFrameTime()\n\n    def get_label_color_map(self):\n        return dict()\n\n    def set_frame_time(self, frame_time):\n        self.frameTime = frame_time\n    def get_semantic_annotation(self):\n        return dict()\n\n    def rotate_dir_vector(self, angle):\n        self.mm_controller.rotate_dir_vector(angle)\n\n   \n    def handle_keyboard_input(self, key):\n        print(\"handle\", key)\n        if key == b\"a\":\n            self.rotate_dir_vector(-10)\n        elif key == b\"d\":\n            self.rotate_dir_vector(10)\n        elif key == b\"w\":\n            self.mm_controller.target_velocity += 1\n            self.mm_controller.target_velocity = min(self.mm_controller.target_velocity, self.mm_controller.max_step_length)\n        elif key == b\"s\":\n            self.mm_controller.target_velocity -= 1\n            self.mm_controller.target_velocity = max(self.mm_controller.target_velocity, 0)\n        elif key == b\"v\":\n            self.toggle_velocity()\n\n    def get_pose(self):\n        return self.mm_controller.get_pose()\n\n    def toggle_velocity(self):\n        self.mm_controller.pose.use_velocity = not self.mm_controller.pose.use_velocity\n        print(\"use_velocity\",self.mm_controller.use_velocity)\n\n\ndef load_mm_database(builder, filename, scale=1, use_batch=False):\n    name = filename.split(\"/\")[-1]\n    scene_object = SceneObject()\n    scene_object.name = name\n    db = MMDatabaseBinaryIO.load(filename)\n    animation_controller = MMControllerVisualization(scene_object, color=get_random_color(), use_batch=use_batch)\n    animation_controller.set_data(db, scale)\n    scene_object.add_component(\"animation_controller\", animation_controller)\n    controller = scene_object._components[\"animation_controller\"]\n    builder._scene.addAnimationController(scene_object, \"animation_controller\")\n    return scene_object\n\ndef load_mm_database_numpy(builder, filename, scale=1, use_batch=False):\n    name = filename.split(\"/\")[-1]\n    scene_object = SceneObject()\n    scene_object.name = name\n    db = MMDatabase()\n    db.load_from_numpy(filename)\n    animation_controller = MMControllerVisualization(scene_object, color=get_random_color(), use_batch=use_batch)\n    animation_controller.set_data(db, scale)\n    scene_object.add_component(\"animation_controller\", animation_controller)\n    controller = scene_object._components[\"animation_controller\"]\n    builder._scene.addAnimationController(scene_object, \"animation_controller\")\n    return scene_object\n    \ndef load_mm_database_numpy_from_data(builder, name, skeleton, data, scale=1):\n    filename = \"tmp\"+os.sep+ name\n    with open(filename, \"wb\") as file:\n        file.write(data)\n    o = builder.create_object_from_file(\"mm\", filename, scale)\n    os.remove(filename)\n    return o\n        \nSceneObjectBuilder.register_file_handler(\"bin.txt\", load_mm_database)\nSceneObjectBuilder.register_file_handler(\"npz.txt\", load_mm_database_numpy)\nSceneObjectBuilder.register_file_handler(\"mm\", load_mm_database_numpy)\nSceneObjectBuilder.register_object(\"mm\", load_mm_database_numpy_from_data)",
            "requirements": "pip install git+https://github.com/eherr/motion-matching"
        },
        "sb3:vis_utils": {
            "dataType": "sb3",
            "engine": "vis_utils",
            "script": "import gym\r\nimport numpy as np\r\nfrom pathlib import Path\r\nfrom vis_utils.scene.components.component_base import ComponentBase\r\nfrom vis_utils.scene.scene_object import SceneObject\r\nfrom vis_utils.scene.scene_object_builder import SceneObjectBuilder\r\nfrom stable_baselines3 import PPO\r\nfrom stable_baselines3.sac import SAC\r\nfrom stable_baselines3.td3 import TD3\r\nfrom stable_baselines3.common.policies import ActorCriticPolicy\r\nfrom stable_baselines3.sac.policies import SACPolicy\r\nfrom stable_baselines3.common.noise import NormalActionNoise\r\nimport torch\r\nfrom torch import nn\r\nimport numpy as np\r\nimport random\r\nfrom functools import partial\r\n\r\n\r\nBEST_MODEL_NAME = \"best_model\"\r\n\r\ndef set_random_seed(seed):\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    print(\"set random seed\",seed)\r\n\r\ndef learning_rate_schedule(progress, lr=3e-4):\r\n    return lr\r\n    #return lr*max(progress**2, 0.2)\r\n\r\n\r\ndef create_ppo(env, **kwargs):\r\n    policy = kwargs.get(\"policy\", ActorCriticPolicy)\r\n    learning_rate = partial(learning_rate_schedule, lr=float(kwargs.get(\"learning_rate\", 3e-4)))\r\n    batch_size = kwargs.get(\"batch_size\", 256)\r\n    n_steps = kwargs.get(\"n_steps\", 2048)\r\n    n_epochs = kwargs.get(\"n_epochs\", 10)\r\n    net_arch = kwargs.get(\"net_arch\", [512, 256, 128])\r\n    activation_fn = kwargs.get(\"activation_fn\", nn.Tanh)\r\n    use_sde = kwargs.get(\"use_sde\", False)\r\n    use_expln = kwargs.get(\"use_expln\", False)\r\n    ent_coef = kwargs.get(\"ent_coef\", 0.0)\r\n    verbose = kwargs.get(\"verbose\", 1)\r\n    optimizer_kwargs = kwargs.get(\"optimizer_kwargs\", None)\r\n    policy_kwargs=dict(net_arch=net_arch, activation_fn=activation_fn, use_expln=use_expln, optimizer_kwargs=optimizer_kwargs)\r\n    tensorboard_log = kwargs.get(\"tensorboard_log\", None)\r\n    return PPO(policy, env, learning_rate=learning_rate, verbose=verbose, batch_size=batch_size, n_steps=n_steps, n_epochs=n_epochs, use_sde=use_sde, ent_coef=ent_coef, policy_kwargs=policy_kwargs, tensorboard_log=tensorboard_log)\r\n\r\n\r\ndef create_sac(env, **kwargs):\r\n    policy = kwargs.get(\"policy\", \"MlpPolicy\")\r\n    learning_rate = partial(learning_rate_schedule, lr=float(kwargs.get(\"learning_rate\", 3e-4)))\r\n    batch_size = kwargs.get(\"batch_size\", 256)\r\n    net_arch = kwargs.get(\"net_arch\", [512, 256, 128])\r\n    activation_fn = kwargs.get(\"activation_fn\", nn.Tanh)\r\n    use_sde = kwargs.get(\"use_sde\", False)\r\n    use_expln = kwargs.get(\"use_expln\", False)\r\n    ent_coef = kwargs.get(\"ent_coef\", 0.0)\r\n    verbose = kwargs.get(\"verbose\", 1)\r\n    optimizer_kwargs = kwargs.get(\"optimizer_kwargs\", None)\r\n    buffer_size = kwargs.get(\"buffer_size\", 2_000_000) \r\n    learning_starts = kwargs.get(\"learning_starts\", 100)\r\n    tau = kwargs.get(\"tau\", 0.005)\r\n    train_freq = kwargs.get(\"train_freq\", 1)\r\n    gamma = kwargs.get(\"gamma\", 0.99) \r\n    policy_kwargs=dict(net_arch=net_arch, activation_fn=activation_fn, use_expln=use_expln, optimizer_kwargs=optimizer_kwargs)\r\n    tensorboard_log = kwargs.get(\"tensorboard_log\", None)\r\n    return SAC(policy, env, learning_rate=learning_rate, verbose=verbose, train_freq=train_freq, gamma=gamma, buffer_size=buffer_size, learning_starts=learning_starts, batch_size=batch_size,  tau=tau,  use_sde=use_sde, ent_coef=ent_coef, policy_kwargs=policy_kwargs, tensorboard_log=tensorboard_log)\r\n\r\ndef create_td3(env, **kwargs):\r\n    policy = kwargs.get(\"policy\", \"MlpPolicy\")\r\n    learning_rate = partial(learning_rate_schedule, lr=float(kwargs.get(\"learning_rate\", 3e-4)))\r\n    batch_size = kwargs.get(\"batch_size\", 256)\r\n    net_arch = kwargs.get(\"net_arch\", [512, 256, 128])\r\n    activation_fn = kwargs.get(\"activation_fn\", nn.Tanh)\r\n    verbose = kwargs.get(\"verbose\", 1)\r\n    optimizer_kwargs = kwargs.get(\"optimizer_kwargs\", None)\r\n    buffer_size = kwargs.get(\"buffer_size\", 2_000_000) \r\n    learning_starts = kwargs.get(\"learning_starts\", 100)\r\n    tau = kwargs.get(\"tau\", 0.005)\r\n    train_freq = kwargs.get(\"train_freq\", 1)\r\n    gamma = kwargs.get(\"gamma\", 0.99) \r\n    policy_kwargs=dict(net_arch=net_arch, activation_fn=activation_fn, optimizer_kwargs=optimizer_kwargs)\r\n    tensorboard_log = kwargs.get(\"tensorboard_log\", None)\r\n    n_actions = env.action_space.shape[0]\r\n    mean = np.zeros(n_actions, np.float32)\r\n    sigma = np.ones(n_actions, np.float32)\r\n    action_noise = NormalActionNoise(mean, sigma)\r\n    return TD3(policy, env, learning_rate=learning_rate, verbose=verbose, train_freq=train_freq, gamma=gamma, buffer_size=buffer_size, learning_starts=learning_starts, batch_size=batch_size, tau=tau, policy_kwargs=policy_kwargs, tensorboard_log=tensorboard_log, action_noise=action_noise)\r\n\r\n\r\n\r\n\r\n\r\nMODEL_CONSTRUCTORS = dict()\r\nMODEL_CONSTRUCTORS[\"ppo\"] = create_ppo\r\nMODEL_CONSTRUCTORS[\"sac\"] = create_sac\r\nMODEL_CONSTRUCTORS[\"td3\"] = create_td3\r\n\r\ndef create_sb3_model(env, **kwargs):\r\n    algorithm = kwargs.get(\"algorithm\", \"ppo\")\r\n    return MODEL_CONSTRUCTORS[algorithm](env, **kwargs)\r\n    \r\n\r\n\r\n\r\nclass AgentController(ComponentBase):\r\n    def __init__(self, scene_object, env, model, deterministic=False, model_path=None):\r\n        ComponentBase.__init__(self, scene_object)\r\n        self.env = env\r\n        self.last_obs, _ = self.env.reset()\r\n        self.model = model\r\n        self.t = 0\r\n        self.active = True\r\n        self.deterministic = deterministic\r\n        self._reload = False\r\n        self.model_path = model_path\r\n     \r\n\r\n    def update(self, dt):\r\n        if self.active:\r\n            done = self.step_env()\r\n            if done:\r\n                self.reset()\r\n                if self._reload:\r\n                    self.reload_model()\r\n                    self._reload = False\r\n                    \r\n\r\n    def trigger_reload(self):\r\n        print(\"trigger reload\")\r\n        self._reload = True\r\n\r\n    def on_modified_dispatch(self, event: FileSystemEvent):\r\n        filename = event.src_path\r\n        path = Path(filename)\r\n        print(\"releoad\", path.stem)\r\n        if str(path.stem) == BEST_MODEL_NAME:\r\n            self._reload = True \r\n\r\n    def step_env(self):\r\n        done = False\r\n        self.env.return_reward_detail = True\r\n        self.deterministic = False\r\n        a, _states = self.model.predict(self.last_obs, deterministic=self.deterministic)\r\n        #print(a)\r\n        self.last_obs, reward, done,_, info = self.env.step(a)\r\n        self.t+=1\r\n        #print(self.t)\r\n        if done:\r\n            print(\"done at\",self.t)\r\n        return done\r\n\r\n    def reload_model(self):\r\n        print(\"reload\", self.model_path)\r\n        self.model.load(self.model_path)\r\n\r\n    def reset(self):\r\n        self.last_obs, _ = self.env.reset()\r\n        self.t = 0\r\n\r\n\r\n\r\ndef create_agent_controller(builder, env, model, model_path=None):\r\n    o = SceneObject()\r\n    o.scene = builder._scene\r\n    agent_controller = AgentController(o, env, model, model_path=model_path)\r\n    builder._scene.addObject(o)\r\n    o.add_component(\"agent_controller\", agent_controller)\r\n    return o\r\n\r\n\r\ndef load_model_from_file(builder, env_id, model_file, visualize=True, **kwargs):\r\n    model = create_sb3_model(env, **kwargs)\r\n    env = gym.make(env_id, render=visualize, visualize=visualize)\r\n    return create_agent_controller(builder, env, model)\r\n\r\n\r\nSceneObjectBuilder.register_object(\"agent_controller\", create_agent_controller)\r\nSceneObjectBuilder.register_file_handler(\"sb3\", load_model_from_file)",
            "requirements": "pip install git+https://github.com/DLR-RM/stable-baselines3"
        },
        "mpm:db": {
            "dataType": "mpm",
            "engine": "db",
            "script": "\nimport numpy as np\nfrom motion_db_interface.model_registry import ModelRegistry\nfrom morphablegraphs.motion_model.motion_primitive_wrapper import MotionPrimitiveModelWrapper\nfrom morphablegraphs.utilities import convert_to_mgrd_skeleton\nfrom motion_database_server.utils import extract_compressed_bson\nfrom anim_utils.animation_data.motion_vector import MotionVector\n\ndef sample_motion_primitive(model_data, skeleton):\n    print(\"sample motion primitive\")\n    data = extract_compressed_bson(model_data)\n    model = MotionPrimitiveModelWrapper()\n    mgrd_skeleton = convert_to_mgrd_skeleton(skeleton)\n    model._initialize_from_json(mgrd_skeleton, data)\n    mv = model.sample(False).get_motion_vector()\n    animated_joints = model.get_animated_joints()\n    new_quat_frames = np.zeros((len(mv), skeleton.reference_frame_length))\n    for idx, reduced_frame in enumerate(mv):\n        new_quat_frames[idx] = skeleton.add_fixed_joint_parameters_to_other_frame(reduced_frame,\n                                                                                    animated_joints)\n    \n    motion_vector = MotionVector()\n    motion_vector.frames = new_quat_frames\n    motion_vector.n_frames = len(new_quat_frames)\n    motion_vector.skeleton = skeleton\n    \n    result_object = motion_vector.to_db_format()\n    return result_object\n\nModelRegistry.register_sample_method(\"mpm\", sample_motion_primitive)",
            "requirements": ""
        }
    },
    "data_transforms": {
        "statistical_modelling": {
            "name": "statistical_modelling",
            "script": "import json\nimport warnings\nimport collections\nimport shutil\nimport numpy as np\nimport scipy.interpolate as si\nfrom anim_utils.animation_data import MotionVector, SkeletonBuilder\nfrom motion_db_interface import get_skeleton_from_remote_db, get_skeleton_model_from_remote_db, get_motion_list_from_remote_db,\\\n                                              get_annotation_by_id_from_remote_db, \\\n                                            get_time_function_by_id_from_remote_db, get_motion_by_id_from_remote_db,  \\\n                                             upload_motion_model_to_remote_db, upload_motion_model_to_remote_db, \\\n                                             upload_cluster_tree_to_remote_db\nfrom morphablegraphs.construction.motion_model_constructor import MotionModelConstructor\nfrom morphablegraphs.construction.utils import get_cubic_b_spline_knots\nfrom morphablegraphs.motion_model.motion_primitive_wrapper import MotionPrimitiveModelWrapper\nfrom morphablegraphs.construction.cluster_tree_builder import FeatureClusterTree\nfrom morphablegraphs.utilities import convert_to_mgrd_skeleton\n\n\ndef get_standard_config():\n    config = dict()\n    config[\"n_basis_functions_spatial\"] = 16\n    config[\"n_spatial_basis_factor\"] = 1.0/5.0\n    config[\"fraction\"] = 0.95\n    config[\"n_basis_functions_temporal\"] = 8\n    config[\"npc_temporal\"] = 3\n    config[\"n_components\"] = None\n    config[\"precision_temporal\"] = 0.99\n    return config\n\ndef generate_training_data(motion_data, animated_joints=None):\n    motions = collections.OrderedDict()\n    sections = collections.OrderedDict()\n    temporal_data = collections.OrderedDict()\n    for name, value in motion_data.items():\n        data = value[\"data\"]\n        motion_vector = MotionVector()\n        motion_vector.from_custom_db_format(data)\n        motions[name] = motion_vector.frames\n        if value[\"section_annotation\"] is not None:#\n            v_type = type(value[\"section_annotation\"])\n            if v_type == list:\n                sections[name] = value[\"section_annotation\"]\n            elif v_type == dict:\n                sections[name] = list()#create_sections_from_annotation(annotation)\n                print(value[\"section_annotation\"])\n                for section_key in value[\"section_annotation\"]:\n                    n_sections  = len(value[\"section_annotation\"][section_key])\n                    if n_sections == 1: # take only the first segment in the list\n                        sections[name].append(value[\"section_annotation\"][section_key][0])\n                    else:\n                        warnings.warn(\"number of annotations \"+str(section_key)+\" \"+str(n_sections))\n            else:\n                warnings.warn(\"type unknown\", name, v_type)\n        if value[\"time_function\"] is not None:\n            temporal_data[name] = value[\"time_function\"]\n    return motions, sections, temporal_data\n\n\ndef get_motion_vectors_from_motion_list(db_url, motion_list, is_processed=False, session=None):\n    count = 1\n    n_motions = len(motion_list)\n    bvh_data = dict()\n    for node_id, name in motion_list:\n        print(\"download motion\", str(count)+\"/\"+str(n_motions), node_id, name, is_processed)\n        motion = get_motion_by_id_from_remote_db(db_url, node_id, is_processed)\n        annotation_str = get_annotation_by_id_from_remote_db(db_url, node_id, is_processed)\n        section_annotation = None\n        if annotation_str != \"\": \n            data = json.loads(annotation_str)\n            if \"sections\" in data:\n                section_annotation = data[\"sections\"]\n        \n        time_function_str = get_time_function_by_id_from_remote_db(db_url, node_id)\n        time_function = None\n        if time_function_str != \"\": \n            time_function = json.loads(time_function_str)\n            #print(\"found str\", node_id, name, type(time_function),  time_function_str)\n            print(time_function)\n            if isinstance(time_function, str) and time_function != \"\":\n                time_function = json.loads(time_function)\n                print(\"WARINING: time function was deserialized to string instead of list\", time_function_str, type(time_function))\n                \n\n        bvh_data[node_id] = dict()\n        bvh_data[node_id][\"data\"] = motion\n        bvh_data[node_id][\"section_annotation\"] = section_annotation\n        bvh_data[node_id][\"time_function\"] = time_function\n        bvh_data[node_id][\"name\"] = name\n        count+=1\n    return bvh_data\n\ndef get_motion_vectors(db_url, collection, skeleton=\"\", is_aligned=False, session=None):\n    motion_list = get_motion_list_from_remote_db(db_url, collection, skeleton, is_aligned)\n    return get_motion_vectors_from_motion_list(db_url, motion_list, is_aligned)\n\ndef load_skeleton_from_db(db_url, skeleton_name, session=None):\n    skeleton_data = get_skeleton_from_remote_db(db_url, skeleton_name, session)\n    if skeleton_data is not None:\n        skeleton = SkeletonBuilder().load_from_custom_unity_format(skeleton_data)\n        skeleton_model = get_skeleton_model_from_remote_db(db_url, skeleton_name, session)\n        skeleton.skeleton_model = skeleton_model\n        return skeleton\n\ndef create_keyframes_from_sections(sections):\n    keyframes = dict()\n    for i, s in enumerate(sections):\n        keyframes[\"contact\"+str(i)] = s[\"end_idx\"]\n    return keyframes\n\n\ndef create_motion_primitive_model(name, skeleton, motion_data, config=None, animated_joints=None, save_skeleton=True, align_frames=True):\n    print(\"create model\", animated_joints)\n    motions, sections, temporal_data  = generate_training_data(motion_data, animated_joints)\n    if config is None:\n        config = get_standard_config()\n\n    constructor = MotionModelConstructor(skeleton, config)\n    constructor.set_motions(motions)\n\n    if align_frames:\n        if len(sections) == len(motions):\n            constructor.set_dtw_sections(sections)\n        else:\n            constructor.set_dtw_sections(None) # ignore\n    else:\n        keyframes = dict()\n        if len(sections) > 0:\n            first_key = list(sections.keys())[0]\n            keyframes = create_keyframes_from_sections(sections[first_key])\n        constructor.set_aligned_frames(motions, keyframes)\n        constructor.set_timewarping(temporal_data)\n    print(\"model\", len(motions), align_frames, len(constructor._aligned_frames))\n    model_data = constructor.construct_model(name, version=3, save_skeleton=save_skeleton, align_frames=align_frames)\n    return model_data\n\n\ndef convert_motion_to_static_motion_primitive(name, frames, skeleton, n_basis=7, degree=3):\n    \"\"\"\n        Represent motion data as functional data, motion data should be narray<2d> n_frames * n_dims,\n        the functional data has the shape n_basis * n_dims\n    \"\"\"    \n    frames = np.asarray(frames)\n    n_frames, n_dims = frames.shape\n    knots = get_cubic_b_spline_knots(n_basis, n_frames)\n    x = list(range(n_frames))\n    coeffs = [si.splrep(x, frames[:, i], k=degree,\n                        t=knots[degree + 1: -(degree + 1)])[1][:-4] for i in range(n_dims)]\n    coeffs = np.asarray(coeffs).T\n\n    data = dict()\n    data[\"name\"] = name\n    data[\"spatial_coeffs\"] = coeffs.tolist()\n    data[\"knots\"] = knots.tolist()\n    data[\"n_canonical_frames\"] = len(frames)\n    data[\"skeleton\"] = skeleton.to_json()\n    return data\n\n\ndef create_cluster_tree_from_model(model_data, n_samples, n_subdivisions_per_level = 4, clustering_method=0):\n    skeleton = SkeletonBuilder().load_from_json_data(model_data[\"skeleton\"])\n    mp = MotionPrimitiveModelWrapper()\n    mp.cluster_tree = None\n    mp._initialize_from_json(convert_to_mgrd_skeleton(skeleton), model_data)\n    data = mp.sample_low_dimensional_vectors(n_samples)\n    n_spatial = mp.get_n_spatial_components()\n    features = data[:, :n_spatial]\n    options = {\"n_subdivisions\": n_subdivisions_per_level,\n                \"clustering_method\": clustering_method,\n                \"use_feature_mean\": False}\n    return FeatureClusterTree(features, data, None, options, [])\n\n\ndef main(**kwargs):\n    exp_name= kwargs[\"exp_name\"]\n    work_dir= kwargs[\"work_dir\"]\n    input_types= kwargs[\"input_types\"]\n    input_ids= kwargs[\"input_ids\"]\n    output_type= kwargs[\"output_type\"]\n    output_id= kwargs[\"output_id\"]\n    skeleton_type= kwargs[\"input_skeleton\"]\n    port= kwargs[\"port\"]\n    user= kwargs[\"user\"]\n    token= kwargs[\"token\"]\n    db_url  = \"http://localhost:\" + str(port) + \"/\"\n    \n    session = {\"user\": user, \"token\": token}\n    input_type= input_types#[0]\n    input_id = input_ids#[0]\n    config = kwargs.get(\"config\", get_standard_config())\n    animated_joints = kwargs.get(\"animated_joints\", None)\n    n_samples = kwargs.get(\"n_samples\", 10000)\n    n_subdivisions_per_level = kwargs.get(\"n_subdivisions_per_level\", 4)\n    motion_data = get_motion_vectors(db_url, input_id, skeleton_type, is_aligned=1)\n    skeleton = load_skeleton_from_db(db_url, skeleton_type)\n    n_motions = len(motion_data)\n    if n_motions > 1:\n        print(\"start modeling\", n_motions)\n        if animated_joints is None:\n            animated_joints = skeleton.animated_joints\n        model_data = create_motion_primitive_model(exp_name, skeleton, motion_data, config, animated_joints, save_skeleton=True, align_frames=False)\n       \n        print(\"finished modelling\")\n        name = exp_name+\"_\"+skeleton_type+\"_\"+str(n_motions)\n        result = upload_motion_model_to_remote_db(db_url, name, input_id, skeleton_type, model_data, config, session)\n        print(\"uploaded model\")\n        if result is not None:\n            new_id = result[\"id\"]\n            tree = create_cluster_tree_from_model(model_data, n_samples, n_subdivisions_per_level)\n            tree_data = dict()\n            tree_data[\"data\"] = tree.data.tolist()\n            tree_data[\"features\"] = tree._features.tolist()\n            tree_data[\"options\"] = tree._options\n            tree_data[\"root\"] = tree.node_to_json()\n            tree_data = json.dumps(tree_data)\n            upload_cluster_tree_to_remote_db(db_url, new_id, tree_data, session)\n\n    elif n_motions == 1:\n        print(\"Create static motion primitive fromn 1 motion\")\n        first_key = list(motion_data.keys())[0]\n        motion_vector = MotionVector()\n        motion_vector.from_custom_db_format(motion_data[first_key][\"data\"])\n\n        config = get_standard_config()\n        n_basis = int(config[\"n_spatial_basis_factor\"]*motion_vector.n_frames)\n        name = exp_name+\"_\"+skeleton_type+\"_\"+str(n_motions)\n        model_data = convert_motion_to_static_motion_primitive(name, motion_vector.frames, skeleton, n_basis=n_basis, degree=3)\n        upload_motion_model_to_remote_db(db_url, name, output_id, skeleton_type, model_data, config, session)\n        print(\"uploaded model\")\n    else:\n        print(\"No motion, found\")\n    # cleanup\n    shutil.rmtree(work_dir, ignore_errors=True)\n\n\n",
            "parameters": "{ \"n_samples\": 10000,\n  \"n_subdivisions_per_level\": 4\n}",
            "requirements": "",
            "outputIsCollection": 0,
            "outputType": "mpm",
            "inputs": [
                [
                    "aligned_motion",
                    1
                ]
            ]
        },
        "create_motion_matching_datastructure": {
            "name": "create_motion_matching_datastructure",
            "script": "import os\nimport shutil\nimport base64\nfrom motion_matching.preprocessing_pipeline import PreprocessingPipeline\nfrom motion_matching.settings import SETTINGS, DEFAULT_FEATURES\nfrom motion_db_interface import ModelDBSession\n\n\ndef main(**kwargs):\n    exp_name= kwargs[\"exp_name\"]\n    work_dir= kwargs[\"work_dir\"]\n    input_types= kwargs[\"input_types\"]\n    input_ids= kwargs[\"input_ids\"]\n    output_type= kwargs[\"output_type\"]\n    output_id= kwargs[\"output_id\"]\n    skeleton_type= kwargs[\"input_skeleton\"]\n    port= kwargs[\"port\"]\n    user= kwargs[\"user\"]\n    token= kwargs[\"token\"]\n    db_url  = \"http://localhost:\" + str(port) + \"/\"\n    \n    session_data = {\"user\": user, \"token\": token}\n    input_type= input_types#[0]\n    input_id = input_ids#[0]\n    print(\"create mm  db\", exp_name, skeleton_type, output_id, output_type)\n    session = ModelDBSession(db_url, session_data)\n    motion_path = work_dir + os.sep + input_type + str(input_id)\n    os.makedirs(motion_path, exist_ok=True)\n    session.export_collection_clips_to_folder(input_id, skeleton_type, motion_path)\n\n    out_filename= work_dir + os.sep + str(output_id) + \".\" + output_type\n    convert_coodinate_system = True\n    kwargs.update(SETTINGS[skeleton_type])\n    pipeline = PreprocessingPipeline(**kwargs)\n    db = pipeline.create_db(motion_path)\n    db.concatenate_data()\n    features_descs =DEFAULT_FEATURES\n    db.calculate_features(features_descs, convert_coordinate_system=convert_coodinate_system, normalize=False)\n    db.calculate_neighbors(normalize=True)\n    db.write_to_numpy(out_filename, False)\n    with open(out_filename+\".npz\", \"rb\") as file:\n        model_data = file.read()\n\n    model_data = base64.b64encode(model_data)\n    model_data = model_data.decode()\n    session.upload_file(output_id, skeleton_type, exp_name, model_data, dataType=output_type)\n    # cleanup\n    shutil.rmtree(work_dir, ignore_errors=True)\n\n\n\n    ",
            "parameters": "{}",
            "requirements": "",
            "outputIsCollection": "",
            "outputType": "mm",
            "inputs": [
                [
                    "motion",
                    1
                ]
            ]
        },
        "spatio_temporal_alignment": {
            "name": "spatio_temporal_alignment",
            "script": "import numpy as np\nimport json\nimport collections\nimport warnings\nfrom anim_utils.animation_data import MotionVector, SkeletonBuilder\nfrom motion_db_interface import get_skeleton_from_remote_db, get_skeleton_model_from_remote_db, get_motion_list_from_remote_db,\\\n                                              get_annotation_by_id_from_remote_db, \\\n                                            get_time_function_by_id_from_remote_db, get_motion_by_id_from_remote_db, upload_motion_to_db, \\\n                                            delete_motion_by_id_from_remote_db\nfrom morphablegraphs.construction.motion_model_constructor import MotionModelConstructor\n\ndef get_motion_vector(skeleton, frames):\n    print(\"generate motion vector\", len(skeleton.animated_joints))\n    frames = np.array(frames)\n    #frames = skeleton.add_fixed_joint_parameters_to_motion(frames)\n    frame_time = skeleton.frame_time\n    mv = MotionVector()\n    mv.frames = frames\n    mv.n_frames = len(frames)\n    mv.skeleton = skeleton\n    return mv\n\n\ndef get_motion_vectors_from_motion_list(db_url, motion_list, is_processed=False, session=None):\n    count = 1\n    n_motions = len(motion_list)\n    bvh_data = dict()\n    for node_id, name in motion_list:\n        print(\"download motion\", str(count)+\"/\"+str(n_motions), node_id, name, is_processed)\n        motion = get_motion_by_id_from_remote_db(db_url, node_id, is_processed)\n        annotation_str = get_annotation_by_id_from_remote_db(db_url, node_id, is_processed)\n        section_annotation = None\n        if annotation_str != \"\": \n            data = json.loads(annotation_str)\n            if \"sections\" in data:\n                section_annotation = data[\"sections\"]\n        \n        time_function_str = get_time_function_by_id_from_remote_db(db_url, node_id)\n        time_function = None\n        if time_function_str != \"\": \n            time_function = json.loads(time_function_str)\n            #print(\"found str\", node_id, name, type(time_function),  time_function_str)\n            print(time_function)\n            if isinstance(time_function, str) and time_function != \"\":\n                time_function = json.loads(time_function)\n                print(\"WARINING: time function was deserialized to string instead of list\", time_function_str, type(time_function))\n                \n\n        bvh_data[node_id] = dict()\n        bvh_data[node_id][\"data\"] = motion\n        bvh_data[node_id][\"section_annotation\"] = section_annotation\n        bvh_data[node_id][\"time_function\"] = time_function\n        bvh_data[node_id][\"name\"] = name\n        count+=1\n    return bvh_data\n\ndef load_skeleton_from_db(db_url, skeleton_name, session=None):\n    skeleton_data = get_skeleton_from_remote_db(db_url, skeleton_name, session)\n    if skeleton_data is not None:\n        skeleton = SkeletonBuilder().load_from_custom_unity_format(skeleton_data)\n        skeleton_model = get_skeleton_model_from_remote_db(db_url, skeleton_name, session)\n        skeleton.skeleton_model = skeleton_model\n        return skeleton\n\ndef get_motion_vectors(db_url, collection, skeleton=\"\", is_aligned=False, session=None):\n    motion_list = get_motion_list_from_remote_db(db_url, collection, skeleton, is_aligned)\n    return get_motion_vectors_from_motion_list(db_url, motion_list, is_aligned)\n\ndef create_sections_from_keyframes(keyframes):\n    sorted_keyframes = collections.OrderedDict(sorted(keyframes.items(), key=lambda t: t[1]))\n    start = 0\n    #end = n_canonical_frames\n    semantic_annotation = collections.OrderedDict()\n    for k, v in sorted_keyframes.items():\n        print(\"set key\",start, v)\n        semantic_annotation[start] = {\"start_idx\":start,  \"end_idx\":v}\n        start = v\n    #semantic_annotation[start] = {\"start_idx\":start,  \"end_idx\":end}\n    return list(semantic_annotation.values())\n\ndef create_motion_vector_from_json(motion_data):\n    motion_vector = MotionVector()\n    motion_vector.from_custom_db_format(motion_data)\n    return motion_vector\n\ndef generate_training_data(motion_data, animated_joints=None):\n    motions = collections.OrderedDict()\n    sections = collections.OrderedDict()\n    temporal_data = collections.OrderedDict()\n    for name, value in motion_data.items():\n        data = value[\"data\"]\n        motion_vector = MotionVector()\n        motion_vector.from_custom_db_format(data)\n        motions[name] = motion_vector.frames\n        if value[\"section_annotation\"] is not None:#\n            v_type = type(value[\"section_annotation\"])\n            if v_type == list:\n                sections[name] = value[\"section_annotation\"]\n            elif v_type == dict:\n                sections[name] = list()#create_sections_from_annotation(annotation)\n                print(value[\"section_annotation\"])\n                for section_key in value[\"section_annotation\"]:\n                    n_sections  = len(value[\"section_annotation\"][section_key])\n                    if n_sections == 1: # take only the first segment in the list\n                        sections[name].append(value[\"section_annotation\"][section_key][0])\n                    else:\n                        warnings.warn(\"number of annotations \"+str(section_key)+\" \"+str(n_sections))\n            else:\n                warnings.warn(\"type unknown\", name, v_type)\n        if value[\"time_function\"] is not None:\n            temporal_data[name] = value[\"time_function\"]\n    return motions, sections, temporal_data\n\ndef create_keyframes_from_sections(sections):\n    keyframes = dict()\n    for i, s in enumerate(sections):\n        keyframes[\"contact\"+str(i)] = s[\"end_idx\"]\n    return keyframes\n\ndef get_standard_config():\n    config = dict()\n    config[\"n_basis_functions_spatial\"] = 16\n    config[\"n_spatial_basis_factor\"] = 1.0/5.0\n    config[\"fraction\"] = 0.95\n    config[\"n_basis_functions_temporal\"] = 8\n    config[\"npc_temporal\"] = 3\n    config[\"n_components\"] = None\n    config[\"precision_temporal\"] = 0.99\n    return config\n\ndef align_motion_data(skeleton, motion_data, config=None, mean_key=None):\n    motions, sections, temporal_data = generate_training_data(motion_data)\n    if config is None:\n        config = get_standard_config()\n    constructor = MotionModelConstructor(skeleton, config)\n    if len(sections) == len(motions):\n        constructor.set_motions(motions)\n        constructor.set_dtw_sections(sections)\n    elif len(sections) > 0: # filter motions by sections\n        _motions = collections.OrderedDict()\n        for key in sections:\n            _motions[key] = motions[key]\n        constructor.set_motions(_motions)\n        constructor.set_dtw_sections(sections) \n    else: # ignore sections\n        constructor.set_motions(motions)\n        constructor.set_dtw_sections(None) \n\n    constructor._align_frames(mean_key)\n\n    # generate from keyframes\n    if len(constructor._keyframes) > 0:\n        #first_key = list(constructor._aligned_frames.keys())[0]\n        #n_canonical_frames = len(constructor._aligned_frames[first_key])\n        key = list(constructor._aligned_frames.keys())[0]\n        n_frames = len(constructor._aligned_frames[key])\n        for key in constructor._keyframes:\n            if constructor._keyframes[key] == -1:\n                constructor._keyframes[key] = n_frames-1\n        meta_info = dict()\n        meta_info[\"sections\"] = create_sections_from_keyframes(constructor._keyframes)\n        meta_info_str = json.dumps(meta_info)\n        print(\"Found\", len(constructor._keyframes), \"keyframes\")\n    else:\n        print(\"No keyframes found\")\n        meta_info_str = \"\"\n\n    aligned_data = dict()\n    for key, frames in constructor._aligned_frames.items():\n        aligned_data[key] = dict()\n\n        aligned_data[key][\"frames\"] = frames # needs to be converted to bvh str before upload\n        aligned_data[key][\"meta_info\"] = meta_info_str\n        aligned_data[key][\"time_function\"] = constructor._temporal_data[key]\n\n    return aligned_data\n\n\ndef main(**kwargs):\n    exp_name= kwargs[\"exp_name\"]\n    work_dir= kwargs[\"work_dir\"]\n    input_types= kwargs[\"input_types\"]\n    input_ids= kwargs[\"input_ids\"]\n    output_type= kwargs[\"output_type\"]\n    output_id= kwargs[\"output_id\"]\n    skeleton_type= kwargs[\"input_skeleton\"]\n    port= kwargs[\"port\"]\n    user= kwargs[\"user\"]\n    token= kwargs[\"token\"]\n    db_url  = \"http://localhost:\" + str(port) + \"/\"\n    \n    session = {\"user\": user, \"token\": token}\n    input_type= input_types#[0]\n    input_id = input_ids#[0]#\n\n    motion_data = get_motion_vectors(db_url, input_id, skeleton_type, is_aligned=0)\n    # delete old data\n    old_aligned_motions = get_motion_list_from_remote_db(db_url, input_id, skeleton_type, is_processed=True)\n    for motion in old_aligned_motions:\n        delete_motion_by_id_from_remote_db(db_url, motion[0], is_processed=True, session=session)\n    skeleton = load_skeleton_from_db(db_url, skeleton_type)\n\n    n_motions = len(motion_data)\n    if n_motions > 1:\n        print(\"start alignment\", n_motions)\n        aligned_data = align_motion_data(skeleton, motion_data)\n        print(\"finished alignment\")\n        for key, data in aligned_data.items():\n            frames = data[\"frames\"]\n            name = motion_data[key][\"name\"] + \"_aligned\"\n            mv = get_motion_vector(skeleton, frames)\n            m_data = mv.to_db_format()\n            try:\n                meta_data = json.loads(data[\"meta_info\"])\n            except:\n                meta_data = dict()\n            meta_data[\"time_function\"] = data[\"time_function\"]\n            meta_data_str = json.dumps(meta_data)\n            upload_motion_to_db(db_url, name, m_data, output_id, skeleton_type, meta_data_str, is_processed=True, session=session)\n        print(\"uploaded aligned data\")\n    elif n_motions == 1:\n        \n        first_key = list(motion_data.keys())[0]\n        name = motion_data[first_key][\"name\"] + \"_aligned\"\n        mdata = motion_data[first_key][\"data\"]\n        meta_data_str = get_annotation_by_id_from_remote_db(db_url, first_key)\n        try:\n            meta_data = json.loads(meta_data_str)\n        except:\n            meta_data = dict()\n        print(\"process\",name)\n        motion = create_motion_vector_from_json(mdata)\n        time_function = list(range(motion.n_frames))\n        meta_data[\"time_function\"] = time_function\n        meta_data_str = json.dumps(meta_data)\n        upload_motion_to_db(db_url, name, mdata, output_id, skeleton_type, meta_data_str, is_processed=True, session=session)\n        print(\"Need more than 1 motion, found\", n_motions)\n    else:\n        print(\"No motions found\")\n\n\n\n\n    \n    ",
            "parameters": "{}",
            "requirements": "",
            "outputIsCollection": null,
            "outputType": "aligned_motion",
            "inputs": [
                [
                    "motion",
                    1
                ]
            ]
        },
        "retargeting": {
            "name": "retargeting",
            "script": "import os\nimport shutil\nfrom anim_utils.retargeting.analytical import Retargeting, generate_joint_map\nfrom anim_utils.animation_data import MotionVector\nfrom motion_db_interface import MotionDBSession\nfrom motion_db_interface import get_motion_by_id_from_remote_db, get_annotation_by_id_from_remote_db, upload_motion_to_db\n\n\ndef retarget_motion_in_db(db_url, retargeting, motion_id, motion_name, collection, skeleton_model_name, is_aligned=False, session=None):\n    motion_data = get_motion_by_id_from_remote_db(db_url, motion_id, is_processed=is_aligned)\n    if motion_data is None:\n        print(\"Error: motion data is empty\")\n        return\n    \n    meta_info_str = get_annotation_by_id_from_remote_db(db_url, motion_id, is_processed=is_aligned)\n    motion_vector = MotionVector()\n    motion_vector.from_custom_db_format(motion_data)\n    motion_vector.skeleton = retargeting.src_skeleton\n    new_frames = retargeting.run(motion_vector.frames, frame_range=None)\n    target_motion = MotionVector()\n    target_motion.frames = new_frames\n    target_motion.skeleton = retargeting.target_skeleton\n    target_motion.frame_time = motion_vector.frame_time\n    target_motion.n_frames = len(new_frames)\n    m_data = target_motion.to_db_format()\n    upload_motion_to_db(db_url, motion_name, m_data, collection, skeleton_model_name, meta_info_str, is_processed=is_aligned, session=session)\n\n\ndef main(**kwargs):\n    exp_name= kwargs[\"exp_name\"]\n    work_dir= kwargs[\"work_dir\"]\n    input_types= kwargs[\"input_types\"]\n    input_ids= kwargs[\"input_ids\"]\n    output_type= kwargs[\"output_type\"]\n    output_id= kwargs[\"output_id\"]\n    input_skeleton= kwargs[\"input_skeleton\"]\n    output_skeleton= kwargs[\"output_skeleton\"]\n    port= kwargs[\"port\"]\n    user= kwargs[\"user\"]\n    token= kwargs[\"token\"]\n    src_scale= kwargs.get(\"src_scale\", 1.0)\n    place_on_ground= kwargs.get(\"place_on_ground\", False)\n    print(kwargs)\n\n    db_url  = \"http://localhost:\" + str(port) + \"/\"\n    session = {\"user\": user, \"token\": token}\n    db_client = MotionDBSession(db_url, session)\n    src_skeleton = db_client.load_skeleton(input_skeleton)\n    dest_skeleton = db_client.load_skeleton(output_skeleton)\n    joint_map = generate_joint_map(src_skeleton.skeleton_model, dest_skeleton.skeleton_model)\n    retargeting = Retargeting(src_skeleton, dest_skeleton, joint_map, src_scale, additional_rotation_map=None, place_on_ground=place_on_ground)\n    input_id = input_ids\n    input_type = input_types\n    motion_list = db_client.get_motion_list(input_id, input_skeleton)\n    for motion in motion_list:\n        motion_id = motion[0]\n        name = motion[1]\n        retarget_motion_in_db(db_url, retargeting, motion_id, name, output_id, output_skeleton, session=session)\n\n    # cleanup\n    if os.path.isdir(work_dir):\n        shutil.rmtree(work_dir, ignore_errors=True)\n\n\n",
            "parameters": "{\n\"src_scale\": 1.0,\n\"place_on_ground\": false\n}",
            "requirements": "echo AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA",
            "outputIsCollection": null,
            "outputType": "motion",
            "inputs": [
                [
                    "motion",
                    1
                ]
            ]
        },
        "mirror": {
            "name": "mirror",
            "script": "import os\nimport shutil\nimport numpy as np\nimport copy\nfrom anim_utils.animation_data import MotionVector\nfrom motion_db_interface import MotionDBSession\nfrom motion_db_interface import get_motion_by_id_from_remote_db, get_annotation_by_id_from_remote_db, upload_motion_to_db\nfrom transformations import euler_from_quaternion, quaternion_from_euler, quaternion_matrix, quaternion_from_matrix, quaternion_multiply\n\n\ndef flip_coordinate_system(q, conv_m):\n    \"\"\"\n    as far as i understand it is assumed that we are already in a different coordinate system\n    so we first flip the coordinate system to the original coordinate system to do the original transformation\n    then flip coordinate system again to go back to the flipped coordinate system\n    this results in the original transformation being applied in the flipped transformation\n    http://www.ch.ic.ac.uk/harrison/Teaching/L4_Symmetry.pdf\n    http://gamedev.stackexchange.com/questions/27003/flip-rotation-matrix\n    https://www.khanacademy.org/math/linear-algebra/alternate_bases/change_of_basis/v/lin-alg-alternate-basis-tranformation-matrix-example\n    \"\"\"\n    m = quaternion_matrix(q)\n    new_m = np.dot(conv_m, np.dot(m, conv_m))\n    return quaternion_from_matrix(new_m)\n\n\ndef swap_parameters(frames, node_names, mirror_map):\n    # mirror joints\n    temp = np.array(frames[:])\n    for node_name in node_names:\n        if node_name in mirror_map.keys():\n            target_node_name = mirror_map[node_name]\n            # print(\"mirror\", node_name, target_node_name)\n            src = node_names.index(node_name) * 4 + 3\n            dst = node_names.index(target_node_name) * 4 + 3\n            frames[:, dst:dst + 4] = temp[:,src:src + 4]\n    return frames\n\ndef flip_root_coordinate_system(q1):\n    \"\"\"\n    http://www.gamedev.sk/mirroring-animations\n    http://www.gamedev.net/topic/599824-mirroring-a-quaternion-against-the-yz-plane/\n    \"\"\"\n\n    conv_m = np.array([[-1, 0, 0, 0],\n                       [0, 1, 0, 0],\n                       [0, 0, -1, 0],\n                       [0, 0, 0, 1]])\n\n\n    m = quaternion_matrix(q1)\n    new_m = np.dot(conv_m, np.dot(m, conv_m))\n    q2 = quaternion_from_matrix(new_m)\n    flip_q = quaternion_from_euler(*np.radians([0, 180, 0]))\n    q2 = quaternion_multiply(flip_q, q2)\n    return q2\n\ndef flip_pelvis_coordinate_system(q):\n    \"\"\"\n    http://www.gamedev.sk/mirroring-animations\n    http://www.gamedev.net/topic/599824-mirroring-a-quaternion-against-the-yz-plane/\n    \"\"\"\n\n    conv_m = np.array([[-1, 0, 0, 0],\n                       [0, 1, 0, 0],\n                       [0, 0, 1, 0],\n                       [0, 0, 0, 1]])\n\n\n    m = quaternion_matrix(q)\n    new_m = np.dot(conv_m, np.dot(m, conv_m))\n    q = quaternion_from_matrix(new_m)\n    flip_q = quaternion_from_euler(*np.radians([0, 0, 0]))\n    q = quaternion_multiply(flip_q, q)\n    return q\n\ndef mirror_animation(node_names,frames,mirror_map, root, conv_m):\n    \"\"\"\n    http://www.gamedev.sk/mirroring-animations\n    http://stackoverflow.com/questions/1263072/changing-a-matrix-from-right-handed-to-left-handed-coordinate-system\n    \"\"\"\n    new_frames = []\n    temp = frames[:]\n    for frame in temp:\n        new_frame = frame[:]\n        #handle root separately\n        new_frame[:3] =[-new_frame[0],new_frame[1],new_frame[2]]\n\n        # bring rotation into different coordinate system\n        for idx, node_name in enumerate(node_names):\n            o = idx * 4 + 3\n            q = copy.copy(new_frame[o:o + 4])\n            if node_name == root:\n                q = flip_pelvis_coordinate_system(q)\n            else:\n                q = flip_coordinate_system(q, conv_m)\n            new_frame[o:o+4] = q\n        new_frames.append(new_frame)\n\n    new_frames = np.array(new_frames)\n    new_frames = swap_parameters(new_frames, node_names, mirror_map)\n    return new_frames\n\n  \n\ndef run_mirror_in_db(db_url, skeleton, motion_id, motion_name, collection, skeleton_name, mirror_map,root, conv_m, session=None):\n    motion_data = get_motion_by_id_from_remote_db(db_url, motion_id, is_processed=False)\n    if motion_data is None:\n        print(\"Error: motion data is empty\")\n        return\n    \n    meta_info_str = get_annotation_by_id_from_remote_db(db_url, motion_id, is_processed=False)\n    motion_vector = MotionVector()\n    motion_vector.from_custom_db_format(motion_data)\n    motion_vector.skeleton = skeleton\n    new_frames = mirror_animation(skeleton.animated_joints, motion_vector.frames, mirror_map, root, conv_m)\n    motion_vector.frames = new_frames\n    m_data = motion_vector.to_db_format()\n    motion_name+= \"_mirrored\"\n    upload_motion_to_db(db_url, motion_name, m_data, collection, skeleton_name, meta_info_str, is_processed=False, session=session)\n\n\ndef main(**kwargs):\n    exp_name= kwargs[\"exp_name\"]\n    work_dir= kwargs[\"work_dir\"]\n    input_types= kwargs[\"input_types\"]\n    input_ids= kwargs[\"input_ids\"]\n    output_type= kwargs[\"output_type\"]\n    output_id= kwargs[\"output_id\"]\n    input_skeleton= kwargs[\"input_skeleton\"]\n    output_skeleton= kwargs[\"output_skeleton\"]\n    port= kwargs[\"port\"]\n    user= kwargs[\"user\"]\n    token= kwargs[\"token\"]\n    left_prefix= kwargs.get(\"left_prefix\", \"Left\")\n    right_prefix= kwargs.get(\"right_prefix\", \"Right\")\n\n    db_url  = \"http://localhost:\" + str(port) + \"/\"\n    session = {\"user\": user, \"token\": token}\n    db_client = MotionDBSession(db_url, session)\n    skeleton = db_client.load_skeleton(input_skeleton)\n    mirror_map = dict( )\n    for n in skeleton.animated_joints:\n        if n.startswith(right_prefix):\n            mirror_map[n] = left_prefix+n[len(right_prefix):] \n        if n.startswith(left_prefix):\n            mirror_map[n] = right_prefix+n[len(left_prefix):] \n    root = skeleton.root\n    print(mirror_map)\n    mirror_vector = np.array([-1.0, 1.0, 1.0])\n    conv_m = np.eye(4)\n    conv_m[:3, :3] = np.diag(mirror_vector)\n    input_id = input_ids\n    input_type = input_types\n    motion_list = db_client.get_motion_list(input_id, input_skeleton)\n    for motion in motion_list:\n        motion_id = motion[0]\n        name = motion[1]\n        run_mirror_in_db(db_url, skeleton, motion_id, name, output_id, output_skeleton, mirror_map, root, conv_m, session=session)\n\n    # cleanup\n    if os.path.isdir(work_dir):\n        shutil.rmtree(work_dir, ignore_errors=True)\n\n\n",
            "parameters": "{\n\"left_prefix\": \"Left\",\n\"right_prefix\": \"Right\"\n}",
            "requirements": "",
            "outputIsCollection": null,
            "outputType": "motion",
            "inputs": [
                [
                    "motion",
                    1
                ]
            ]
        },
        "translate_motion": {
            "name": "translate_motion",
            "script": "import os\nimport shutil\nimport numpy as np\nimport copy\nfrom anim_utils.animation_data import MotionVector\nfrom motion_db_interface import MotionDBSession\nfrom motion_db_interface import get_motion_by_id_from_remote_db, get_annotation_by_id_from_remote_db, upload_motion_to_db\nfrom transformations import euler_from_quaternion, quaternion_from_euler, quaternion_matrix, quaternion_from_matrix, quaternion_multiply\n\n\ndef translate_motion_in_db(db_url, skeleton, motion_id, motion_name, collection, skeleton_name, offset,  session=None):\n    motion_data = get_motion_by_id_from_remote_db(db_url, motion_id, is_processed=False)\n    if motion_data is None:\n        print(\"Error: motion data is empty\")\n        return\n    \n    meta_info_str = get_annotation_by_id_from_remote_db(db_url, motion_id, is_processed=False)\n    motion_vector = MotionVector()\n    motion_vector.from_custom_db_format(motion_data)\n    motion_vector.frames[:,:3] += offset\n    motion_vector.skeleton = skeleton\n    m_data = motion_vector.to_db_format()\n    motion_name+= \"_translated\"\n    upload_motion_to_db(db_url, motion_name, m_data, collection, skeleton_name, meta_info_str, is_processed=False, session=session)\n\n\ndef main(**kwargs):\n    exp_name= kwargs[\"exp_name\"]\n    work_dir= kwargs[\"work_dir\"]\n    input_types= kwargs[\"input_types\"]\n    input_ids= kwargs[\"input_ids\"]\n    output_type= kwargs[\"output_type\"]\n    output_id= kwargs[\"output_id\"]\n    input_skeleton= kwargs[\"input_skeleton\"]\n    output_skeleton= kwargs[\"output_skeleton\"]\n    port= kwargs[\"port\"]\n    user= kwargs[\"user\"]\n    token= kwargs[\"token\"]\n    offset= kwargs.get(\"offset\", [0,0,0])\n    offset = np.array(offset)\n\n    db_url  = \"http://localhost:\" + str(port) + \"/\"\n    session = {\"user\": user, \"token\": token}\n    db_client = MotionDBSession(db_url, session)\n    skeleton = db_client.load_skeleton(input_skeleton)\n    \n    input_id = input_ids\n    input_type = input_types\n    motion_list = db_client.get_motion_list(input_id, input_skeleton)\n    for motion in motion_list:\n        motion_id = motion[0]\n        name = motion[1]\n        translate_motion_in_db(db_url, skeleton , motion_id, name, output_id, output_skeleton, offset , session=session)\n\n    # cleanup\n    if os.path.isdir(work_dir):\n        shutil.rmtree(work_dir, ignore_errors=True)\n\n\n",
            "parameters": "{\n    \"offset\": [0,0,0]\n}",
            "requirements": "",
            "outputIsCollection": null,
            "outputType": "motion",
            "inputs": [
                [
                    "motion",
                    1
                ]
            ]
        }
    }
}